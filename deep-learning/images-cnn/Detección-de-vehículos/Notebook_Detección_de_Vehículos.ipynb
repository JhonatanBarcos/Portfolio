{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bukWo1QXbWE2"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOXJIVwb-033"
      },
      "source": [
        "## Aprendizaje Profundo para el Análisis de Imágenes\n",
        "## Grados en Ingeniería de Telecomunicación\n",
        "\n",
        "# PROYECTO 2B:\n",
        "# DETECCIÓN DE OBJETOS CON FASTER-RCNN\n",
        "\n",
        "<center><img src='https://www.uc3m.es/ss/Satellite?blobcol=urldata&blobkey=id&blobtable=MungoBlobs&blobwhere=1371614316787&ssbinary=true' width=400 /></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSazWDcebWFC"
      },
      "source": [
        "## Objetivo\n",
        "\n",
        "El propósito de este laboratorio es que el alumno se familiarice con una red neuronal convolucional de proposición de regiones (RCNN) para la detección automática de clases de objetos, en la que sobre una imagen se localizan espacialmente una serie de objetos, y se clasifican dichos objetos de acuerdo a unas clases predefinidas.\n",
        "\n",
        "En concreto, se trabajará sobre la red Faster-RCNN, la primera red que aúna la estrategia de proposición de regiones automática (es decir, a partir de la imagen de entrada se proponen regiones susceptibles de contener objetos) con la clasificación de los mismos. En nuestro caso, esta red se aplicará a algunas de las clases de la base de datos PASCAL VOC para detección de objetos.\n",
        "\n",
        "Este tutorial es una adaptación del tutorial disponible en https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAteaecfbWFH"
      },
      "source": [
        "## Referencias\n",
        "\n",
        "- [1] Faster-RCNN. https://arxiv.org/abs/1706.05587\n",
        "- [2] PASCAL VOC 2012 dataset. http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html\n",
        "- [3] https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection\n",
        "- [4] https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/\n",
        "- [5] Fast-RCNN y ROIPooling: https://arxiv.org/abs/1504.08083\n",
        "- [6] https://en.wikipedia.org/wiki/File:RoI_pooling_animated.gif\n",
        "- [7] FPN: https://arxiv.org/abs/1612.03144"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDa2gUbNbWFI"
      },
      "source": [
        "## Antes de empezar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUmeqnEmbWFJ"
      },
      "source": [
        "Antes de empezar, necesita configurar algunas cosas en caso de que vaya a utilizar Google Colab. En particular, necesita descomprimir los archivos de la práctica en una carpeta en Drive y cambiar el directorio de trabajo al de dicha carpeta. Para ello, ejecute el siguiente código:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9kpu0KmbWFK",
        "outputId": "477e2910-83b3-4f44-de69-2be890f5bfb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/analisis_imagenes/LAB2\n"
          ]
        }
      ],
      "source": [
        "#SOLO PARA EJECUTAR EN GOOGLE COLAB. Haga esto una única vez sobre la máquina en la que quiera ejecutar su código y luego puede comentar el código\n",
        "from shutil import copyfile\n",
        "from google.colab import drive\n",
        "import os, sys\n",
        "drive.mount('/content/drive')\n",
        "#Nos situamos en la carpeta en la que vamos a trabajar, modificar línea si es necesario\n",
        "%cd /content/drive/MyDrive/analisis_imagenes/LAB2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs4jbTSCbWFQ"
      },
      "source": [
        "Además, si quiere ejecutar el código con soporte a GPU, en Google Colab vaya a `Entorno de ejecución->Cambiar tipo entorno de ejecución` y seleccione GPU en `acelerador por hardware`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "883Ll5P8bWFR"
      },
      "source": [
        "## Parte 1. Fundamento teórico."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkXSBkRsbWFS"
      },
      "source": [
        "### Detección de objetos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KMr3sApbWFT"
      },
      "source": [
        "En el caso de la detección de objetos, un objeto se define de acuerdo a una _bounding box_ (una caja que engloba al objeto completo). La *bounding box* se define de acuerdo a su centro, anchura y altura (normalizados al tamaño de la imagen). Para una imagen de ejemplo y el objeto \"gato\":\n",
        "<img src=\"https://www.tsc.uc3m.es/~matorres/images/apai/detection/cs.png\">\n",
        "Cada uno de estos objetos también llevará asociada una etiqueta de acuerdo a las categorías predeterminadas. Si por ejemplo en nuestro caso, la categoría \"perro\" se define como la clase 1 y la categoría \"gato\" como la clase 2, la etiqueta de este objeto será 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2BKQdz9bWFV"
      },
      "source": [
        "### Faster-RCNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNg1OxUibWFV"
      },
      "source": [
        "Aunque se recomienda echar un vistazo al artículo sobre Faster-RCNN [1], en esta sección se van a explicar los conceptos más importantes de la red que son necesarios para el desarrollo de la práctica."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dvc8TgabWFW"
      },
      "source": [
        "Faster-RCNN es un _framework_ que adapta cualquier red convolucional dedicada a la clasificación (dada una imagen, encontrar una categoría que describa el contenido total de la imagen) a la detección de objetos (en una imagen pueden coexistir objetos distintos). Para ello, a partir de un _backbone_ inicial (las capas destinadas a la extracción de características) de cualquier red de clasificación, propone dos bloques: la red de proposición de regiones y el *RoI pooling* para clasificación.\n",
        "\n",
        "<img src=\"https://www.tsc.uc3m.es/~matorres/images/apai/detection/faster.jpeg\">\n",
        "\n",
        "Por tanto, la arquitectura de Faster-RCNN parte de un mapa de características proveniente de las capas convolucionales, típicamente con _stride_ 16: es decir, si el tamaño de la imagen de entrada es H x W, los mapas de características sobre los que trabaja la red de proposición de regiones son de tamaño H/16 x W/16. A partir de este mapa de características se incluyen dos módulos: una sub-red para proposición de regiones que se ejecuta en primer lugar; y un segundo módulo convolucional que a partir de las características y las regiones propuestas clasifica cada una de ellas de acuerdos a unas clases predeterminadas, haciendo uso de la estrategia de *RoI pooling*.\n",
        "\n",
        "#### Red de proposición de regiones\n",
        "\n",
        "El objetivo de este bloque es extraer las regiones de la imagen susceptibles de contener un objeto, es decir, proponer una serie de _bounding boxes_ candidatas. Para ello, en cada localización espacial de estos mapas de características se consideran _k_ _anchors_ invariantes a la traslación (regiones de tamaño y relación de aspecto fijas). Típicamente, _k_=9 (se consideran 3 tamaños distintos: 128, 256 y 512 píxeles; y las relaciones de aspecto 1:1, 1:2 y 2:1).  \n",
        "\n",
        "He aquí un ejemplo visual de la localización espacial del conjunto de _anchors_ invariantes a la traslación sobre una imagen de entrada.\n",
        "\n",
        "<img src=\"https://www.tsc.uc3m.es/~matorres/images/apai/detection/anchor_centers.png\">\n",
        "\n",
        "Y el aspecto visual de los _anchors_ sobre la misma, en la localización de la persona, en el centro de la figura, así como sobre toda la imagen (el total de _anchors_ para la imagen).\n",
        "\n",
        "<img src=\"https://www.tsc.uc3m.es/~matorres/images/apai/detection/anchors-progress.png\">\n",
        "\n",
        "Ahora bien, del gran número de _anchors_ con los que se cuenta ($k \\cdot H/16 \\cdot W/16$), sólo algunos coinciden con la posición y tamaño real de los objetos en la imagen. Para distinguir estos _anchors_ del total de los mismos se tiene una función de pérdida $L_{objectness}$ que se ocupa de clasificar los _anchors_ que tienen una IoU - _Intersection over Union_ - elevada con algún objeto del _ground truth_ como _anchors_ buenos. Para más detalles, véase [1].\n",
        "\n",
        "Finalmente, los _anchors_ no pueden representar nuestra predicción (son poco precisos a la hora de localizar objetos). Sin embargo, los _anchors_ representan el punto de partida aproximado para ajustar la predicción al objeto que se quiere detectar. Se puede considerar cada _bounding box_ predicha como una ligera desviación frente al _anchor_.\n",
        "\n",
        "Así, Faster-RCNN considera la _bounding box_ predicha como la diferencia entre la _bounding box_ del objeto real y la del _anchor_ seleccionado para dicho objeto. La función de pérdida $L_{reg}$ cuantifica esta pérdida de acuerdo a los parámetros de la _bounding box_ que se han definido anteriormente: centro, anchura y altura, como se muestra en la siguiente imagen. Para más detalles, véase [1].\n",
        "\n",
        "<img src=\"https://www.tsc.uc3m.es/~matorres/images/apai/detection/reg.png\">\n",
        "\n",
        "Por tanto, la función de pérdida $L_{objectness}$ se ocupa de encontrar el mejor _anchor_ para un objeto dado, mientras que la función $L_{reg}$ adapta ese _anchor_ al tamaño original del objeto.\n",
        "\n",
        "#### _RoI pooling_ y clasificación\n",
        "\n",
        "Hasta ahora, se han definido los objetos de manera agnóstica: no se ha prestado atención a su categoría. Una vez que la red de proposición de regiones es capaz de ofrecer una serie de _bounding boxes_ candidatas, es momento de obtener su categoría. Ahora bien, las regiones que propone la red pueden ser de tamaños muy diferentes, pero para ejecutar una red neuronal para clasificación el tamaño de los mapas de características para todas las imágenes debe ser el mismo. Para transformar la información que proporcionan los mapas de características que propone la red de proposición de regiones -de distinto tamaño- a un tamaño fijo (por ejemplo, 7x7), se utiliza el _RoI pooling_.\n",
        "\n",
        "La capa de _RoI pooling_ transforma las características dentro de cualquier región válida de la imagen de tamaño $H_{bb}$ x $W_{bb}$ en un _grid_ de tamaño fijo $H_{fix}$ x $W_{fix}$. Para ello, se divide la región de tamaño $H_{bb}$ x $W_{bb}$ en $H_{fix}$ x $W_{fix}$ celdas, cada una de ellas con tamaño $H_{bb}/H_{fix}$ x $W_{bb}/W_{fix}$. Tras esto, se aplica un _max pooling_ en cada celda para obtener el valor máximo de la misma, que es el valor que aparecerá en la posición de dicha celda en el _grid_ de salida. La siguiente animación muestra un ejemplo de _RoI pooling_ sobre un mapa de características, con un tamaño de *bounding box* de 5x7,  para un tamaño de salida de 2x2.\n",
        "\n",
        "<img src=\"https://www.tsc.uc3m.es/~matorres/images/apai/detection/RoI_pooling_animated.gif\">\n",
        "\n",
        "Una vez se tienen las regiones con un tamaño fijo, la red clasifica las mismas de acuerdo a la categoría de objeto. La función de pérdida multiclase $L_{cls}$ se ocupa de esto. Asimismo, la red refina las _bounding boxes_ para cada clase de objetos (la función de pérdida $L_{reg2}$ se ocupa de esto). Mientras que en el primer caso la regresión era agnóstica (independiente de la clase del objeto), en este caso la regresión tiene en cuenta la categoría de cada objeto.\n",
        "\n",
        "A continuación se muestra un posible resultado de la detección de objetos sobre una imagen de entrada: *bounding box* + predicción de la clase de objeto (con su verosimilitud).\n",
        "\n",
        "<img src=\"https://www.tsc.uc3m.es/~matorres/images/apai/detection/result.png\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0cG8GesNrXj"
      },
      "source": [
        "### FPN\n",
        "\n",
        "Para las redes de detección de objetos, la detección a diferentes escalas es un reto, en particular para objetos pequeños. La estrategia tradicional para ello es la construcción de una pirámide para la imagen a diferentes escalas (véase la figura de abajo [7]), y la aplicación del mismo procedimiento de detección para las diferentes escalas. Sin embargo, en el caso de redes neuronales el procesamiento de diferentes versiones de la misma imagen exige memoria y tiempo. Sin embargo, este procesamiento multiescala se puede aplicar a las características de alto nivel de alguna capa de la red (ya procesadas), lo que es más eficiente (además su dimensión espacial es menor).\n",
        "\n",
        "<img src=\"https://www.tsc.uc3m.es/~matorres/images/apai/detection/pyramid.jpeg\">\n",
        "\n",
        "Hay que tener en cuenta que los mapas de características cercanos a la entrada no son útiles para la detección de objetos, ya que están compuesto de estructuras a bajo nivel (bordes) de la imagen. Por ello, frecuentemente, de manera previa a la proposición de regiones (RPN) se aplica una *Feature Pyramid Network* (FPN), un extractor de características que genera mapas de características de diferente tamaño (multiescala) con información diferente para cada escala, como se muestra en la figura posterior [7].\n",
        "\n",
        "<img src=\"https://www.tsc.uc3m.es/~matorres/images/apai/detection/fpn.png\">\n",
        "\n",
        "Sobre cada uno de estos mapas de características, la RPN propone los *anchors*. En concreto, en función de la resolución espacial de cada mapa de características la RPN utiliza distintos *anchors* invariantes a la traslación: la RPN propone los *anchors* invariantes a la traslación de tamaño más pequeño sobre el mapa de características de mayor resolución y los *anchors* invariantes a la traslación más grandes sobre las localizaciones de los mapas de menor resolución. Esto tiene sentido desde el punto de vista lógico: sobre los mapas con mayor información de bajo nivel se colocan los *anchors* pequeños para buscar objetos de pequeño tamaño, y sobre los mapas con información a alto nivel se colocan los *anchors* de mayor tamaño, que buscan objetos que suponen un gran porcentaje del área de la imagen.\n",
        "\n",
        "Para más información, véase [7]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5sXEE1mbWFX"
      },
      "source": [
        "### Medidas de evaluación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bR5YRYv9bWFZ"
      },
      "source": [
        "El rendimiento del sistema en detección se analiza en términos de la medida F1. Sin embargo, para definir esta medida hay que definir antes la precisión y el *recall*. Cada una de estas medidas se definen a continuación.\n",
        "\n",
        "La __precisión__, para el caso de detección de objetos, representa el porcentaje de objetos correctamente detectados del total de objetos detectados (correctos e incorrectos). Se calcula como:\n",
        "\n",
        "\\begin{equation}\n",
        "P=\\frac{TP}{TP+FP},\n",
        "\\end{equation}\n",
        "\n",
        "donde $TP$ hace referencia al número de *true positives* (objetos correctamente detectados) y $FP$ al número de *false positives* (objetos incorrectamente detectados).\n",
        "\n",
        "El __*recall*__, por su parte, representa el porcentaje de objetos correctamente detectados del total de objetos presentes en la imágenes. Se calcula como:\n",
        "\n",
        "\\begin{equation}\n",
        "R=\\frac{TP}{TP+FN},\n",
        "\\end{equation}\n",
        "\n",
        "donde $FN$ representa el número de *false negatives* (objetos presentes en las imágenes pero no detectados).\n",
        "\n",
        "Ahora bien, la red proporciona una serie de *bounding boxes* sobre la imagen que pueden coincidir en mayor o menor medida con el *ground truth*. Hay que establecer un __criterio para considerar una predicción como correcta__. Este se suele establecer en la calidad de segmentación, mediante la medida *Intersection over Union*, __IoU__, también denominada *Jaccard Index* (JI). La medida $IoU$ mide la similitud entre dos regiones $A$ y $B$ como:\n",
        "\n",
        "\\begin{equation}\n",
        "IoU=\\frac{A \\cap B}{A \\cup B}\n",
        "\\end{equation}\n",
        "\n",
        "siendo $\\cap$ la intersección entre las regiones (area común) y $\\cup$ la unión o área total que cubren entre ambas. Se considera un umbral mínimo de $IoU_{th}=0.5$ o $IoU_{th}=0.7$ para considerar una detección como correcta. A continuación se puede ver un ejemplo de la medida $IoU$ sobre *bounding boxes*, como se utilizará en nuestra arquitectura de detección de objetos.\n",
        "\n",
        "<img src=\"https://www.tsc.uc3m.es/~matorres/images/apai/detection/iou.png\" width=\"500 pix\">\n",
        "\n",
        "\n",
        "De igual modo, la medida $IoU$ se puede utilizar para establecer la calidad en la segmentación de instancias de objetos.\n",
        "\n",
        "Finalmente, una vez que se han descrito estas medidas se pueden definir la medida F1. El F1-*score* considera tanto la precisión como el *recall* para obtener una media armónica ponderada entre los mismos tal que:\n",
        "\n",
        "\\begin{equation}\n",
        "F1=2\\cdot \\frac{P\\cdot R}{P+R},\n",
        "\\end{equation}\n",
        "\n",
        "De este modo, esta medida resume en una sola cifra el rendimiento de nuestro detector de objetos para un __punto de operación__ dado. La red proporciona una serie de *bounding boxes* con un __*score*__ asociado, que nos indica cómo de probable es que en dicha región exista un objeto de cierta clase. Ahora bien, la precisión, el *recall* y la medida F1 no tienen en cuenta este *score* para el cálculo, y estas medidas dependerán del __punto de operación__ que se elija para el sistema: frecuentemente, se descartan aquellas regiones con un *score* bajo ($<0.5$). Esto se analizará a lo largo de la práctica."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJGnXdBobWFZ"
      },
      "source": [
        "### Base de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-u9w5oLbWFa"
      },
      "source": [
        "La base de datos sobre la que se trabajará en la práctica es un subconjunto de la base de datos PASCAL VOC 2012 [2]. Esta base de datos contiene imágenes con diversos objetos pertenecientes a 20 categorías:\n",
        "- __Person__: _person_\n",
        "- __Animal__: _bird, cat, cow, dog, horse, sheep_\n",
        "- __Vehicle__: _aeroplane, bicycle, boat, bus, car, motorbike, train_\n",
        "- __Indoor__: _bottle, chair, dining table, potted plant, sofa, tv/monitor_\n",
        "\n",
        "Durante esta práctica se va a trabajar sobre el conjunto __Indoor__. En concreto, se detectarán objetos de las clases: _bottle, chair_ y _dining table_. Las imágenes se proporcionan en sendas carpetas para entrenamiento y test. Los conjuntos se distribuyen de la siguiente manera:\n",
        "- Entrenamiento: 234 imágenes, en las cuales existen 90 objetos de la clase *bottle*, 125 de la clase *chair*, 36 de la clase *dining table* y 40 de la clase *sofa*.\n",
        "- Test: 68 imágenes, en las cuales existen 36 objetos de la clase *bottle*, 52 de la clase *chair* y 15 de la clase *dining table* y 27 de la clase *sofa*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDLKHGxJbWFb"
      },
      "source": [
        "## Parte 2. Implementación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAJk5jQubWFc"
      },
      "source": [
        "En primer lugar, se importan las librerías necesarias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wf0s39kvbWFd",
        "outputId": "20da3336-5c46-4bf4-8663-609df2fa1dac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Seed:  999\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from PIL import Image, ImageFile, ImageDraw\n",
        "import cv2\n",
        "import random\n",
        "# Set random seed for reproducibility\n",
        "manualSeed = 999\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "# torch uses some non-deterministic algorithms\n",
        "torch.backends.cudnn.enabled = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3HVnwz0bWFi"
      },
      "source": [
        "### Entradas\n",
        "\n",
        "Se definen algunas entradas para la ejecución:\n",
        "\n",
        "-  **data_dir** - el directorio raíz de la base de datos, que se describe posteriormente.\n",
        "-  **num_workers** - el número de hebras para cargar los datos con la clase DataLoader.\n",
        "-  **batch_size** - el tamaño de _batch_.\n",
        "-  **num_classes** - el número de clases que detectar.\n",
        "-  **class_names** - los nombres de las clases que detectar.\n",
        "-  **num_epochs** - número de _epochs_ para el entrenamiento de la red.\n",
        "-  **step_size** - número de *epochs* tras los cuales se reduce el *learning rate* en un factor 0.1.\n",
        "-  **lr** - *learning rate* inicial.\n",
        "-  **device** - el dispositivo (GPU o CPU) para la ejecución."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Z5R8rQ4-bWFj"
      },
      "outputs": [],
      "source": [
        "data_dir = \"myVOC4\"\n",
        "num_workers=8         # to debug, fix num_workers=0\n",
        "batch_size = 1        # Training and test (if we use 1 we don't need to resize the images)\n",
        "num_classes=5         # Number of classes\n",
        "class_names=['background','bottle','chair','dining_table','sofa']\n",
        "num_epochs = 8\n",
        "step_size=num_epochs/3\n",
        "lr=0.001\n",
        "# Detect if we have a GPU available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud-4uoN1bWFn"
      },
      "source": [
        "### Librería modificada\n",
        "\n",
        "Para esta práctica se va a utilizar una versión modificada de la librería torchvision, que denominaremos torchvision_05. Esta versión modificada permite entrenar nuestra red de detección con imágenes que no contengan ningún objeto, mientras que la versión original no lo permite. Asimismo, nos proporciona la información de los _anchors_ seleccionados para cada objeto durante la inferencia. Importamos los módulos necesarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RqIzieQbbWFn"
      },
      "outputs": [],
      "source": [
        "import torchvision_05\n",
        "from torchvision_05.models.detection.rpn import AnchorGenerator\n",
        "from torchvision_05.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN\n",
        "from torchvision_05.models.detection.backbone_utils import resnet_fpn_backbone\n",
        "import torchvision.transforms.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-OXXBtIbWFs"
      },
      "source": [
        "### Red\n",
        "\n",
        "La red a utilizar será una versión de Faster-RCNN sobre ResNet-50. En concreto, se utilizan los 4 primeros bloques de esta red, compuestos de capas convolucionales, normalización de batch (cuyos parámetros se congelan) y capas no lineales. A esto se añade una FPN, _Feature Pyramid Network,_ que extrae características de los mapas resultantes de la red a distintas escalas. Tras este *backbone*, se colocan los módulos propios de Faster-RCNN: la red de proposición de regiones y el _RoI pooling_ y la clasificación.\n",
        "\n",
        "A continuación se incluye la función que construye la red."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "34rrV7QCbWFv"
      },
      "outputs": [],
      "source": [
        "def get_model_detection(num_classes):\n",
        "    # load a Faster RCNN model pre-trained on COCO\n",
        "    model = torchvision_05.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "    # get number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67kuRikSbWF0"
      },
      "source": [
        "Cargamos el modelo. Familiarícese con las distintas capas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBMIychIbWF0",
        "outputId": "c4de3442-7d1b-4ed8-9cec-62137f55b8f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FasterRCNN(\n",
            "  (transform): GeneralizedRCNNTransform()\n",
            "  (backbone): BackboneWithFPN(\n",
            "    (body): IntermediateLayerGetter(\n",
            "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "      (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "      (layer1): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): FrozenBatchNorm2d(256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (layer2): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): FrozenBatchNorm2d(512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (3): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (layer3): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): FrozenBatchNorm2d(1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (3): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (4): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (5): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (layer4): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): FrozenBatchNorm2d(2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (fpn): FeaturePyramidNetwork(\n",
            "      (inner_blocks): ModuleList(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "        (2): Conv2dNormActivation(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "        (3): Conv2dNormActivation(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (layer_blocks): ModuleList(\n",
            "        (0-3): 4 x Conv2dNormActivation(\n",
            "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (extra_blocks): LastLevelMaxPool()\n",
            "    )\n",
            "  )\n",
            "  (rpn): RegionProposalNetwork(\n",
            "    (anchor_generator): AnchorGenerator()\n",
            "    (head): RPNHead(\n",
            "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): RoIHeads(\n",
            "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=[0, 1, 2, 3], output_size=(7, 7), sampling_ratio=2)\n",
            "    (box_head): TwoMLPHead(\n",
            "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    )\n",
            "    (box_predictor): FastRCNNPredictor(\n",
            "      (cls_score): Linear(in_features=1024, out_features=5, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=20, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model_ft = get_model_detection(num_classes)\n",
        "print(model_ft)\n",
        "# Send the model to GPU\n",
        "model_ft = model_ft.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1O2DXZEbWF4"
      },
      "source": [
        "### Base de datos\n",
        "\n",
        "A continuación se describe la base de datos. Para simplificar su tratamiento posterior, la base de datos se proporciona en forma de imágenes (`images`) y máscaras para cada una de las instancias de objetos (`instances`): es decir, para cada imagen se tienen tantas máscaras como objetos existan en la misma, con la segmentación de cada uno de ellos. Asimismo, la categoría de cada objeto va codificada en el nombre de la imagen. Además se proporcionan la segmentación semántica `classes` y la segmentación de instancias `masks` por si resultan útiles.\n",
        "\n",
        "Mediante este formato realizar el proceso de *data augmentation* es más sencillo, ya que basta con aplicar las mismas transformaciones (traslaciones, rotaciones, etc.) a la imagen original y a la máscara y después extraer las _bounding boxes_ que engloban cada objeto (como hace el fragmento de código de abajo); en lugar de aplicar transformaciones coherentes a la imagen y las coordenadas de las *bounding boxes*.\n",
        "\n",
        "A continuación se define la clase que implementa la carga la base de datos.\n",
        "\n",
        "****IMPORTANTE**: las imágenes de entrada no se normalizan. El propio _framework_ realiza la normalización internamente con la media de ImageNet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "VI55x40abWF5",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "class myVOCDataset(object):\n",
        "    def __init__(self, root, train, data_augm):\n",
        "        self.root = root\n",
        "        self.train = train\n",
        "        self.data_augm = data_augm\n",
        "        # load all image files, sorting them to\n",
        "        # ensure that they are aligned\n",
        "        if (self.train):\n",
        "            self.imgs = [os.path.join(root,name)\n",
        "                 for root, dirs, files in os.walk(root)\n",
        "                 for name in files if ('train'+os.path.sep+'images' in root)\n",
        "                 if name.lower().endswith(\".jpg\")]\n",
        "        else:\n",
        "            self.imgs = [os.path.join(root,name)\n",
        "                 for root, dirs, files in os.walk(root)\n",
        "                 for name in files if ('test'+os.path.sep+'images' in root)\n",
        "                 if name.lower().endswith(\".jpg\")]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load images ad masks\n",
        "        img_path = self.imgs[idx]\n",
        "        ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Independent annotations for each object\n",
        "        file=self.imgs[idx].split(os.path.sep)[-1]\n",
        "        self.masks = [os.path.join(self.root,name)\n",
        "                        for self.root, dirs, files in os.walk(self.imgs[idx].replace('images','instances').replace(file,''))\n",
        "                        for name in files\n",
        "                        if (name.startswith(file[:-4]+'_'))]\n",
        "        if (not self.masks):\n",
        "            masks=np.array(Image.new('L', img.size))\n",
        "            labels=np.zeros((0,),dtype=np.int64)\n",
        "            num_objs=0\n",
        "        else:\n",
        "            mask=np.zeros((len(self.masks),img.size[1],img.size[0]),dtype=np.uint8)\n",
        "            labels=np.zeros((len(self.masks),),dtype=np.int64)\n",
        "            for j in range(len(self.masks)):\n",
        "                mask[j,:,:]=np.array(Image.open(self.masks[j]))\n",
        "                labels[j]=int(self.masks[j].split('_')[-1][:-4])\n",
        "            num_objs = len(mask)\n",
        "            masks=np.asarray(mask)\n",
        "\n",
        "        if (self.train and self.data_augm):\n",
        "            # Implement your data augmentation\n",
        "            pass\n",
        "\n",
        "        boxes = []\n",
        "\n",
        "        for i in range(num_objs):\n",
        "            pos = np.where(masks[i])\n",
        "            xmin = np.min(pos[1])\n",
        "            xmax = np.max(pos[1])\n",
        "            ymin = np.min(pos[0])\n",
        "            ymax = np.max(pos[0])\n",
        "            # Get aspect ratio\n",
        "            W=xmax-xmin\n",
        "            H=ymax-ymin\n",
        "            if (W==0 or H==0):\n",
        "                labels=np.delete(labels,i)\n",
        "            else:\n",
        "                boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        # convert everything into a torch.Tensor\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        num_objs = boxes.size(0)\n",
        "        # get the labels\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "        image_id = torch.tensor([idx])\n",
        "        # We need the area to filter the empty images\n",
        "        if (not boxes.size()==torch.Size([0])):\n",
        "            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        else:\n",
        "            area = torch.tensor([0.])\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "\n",
        "        img=F.to_tensor(img)\n",
        "        return img, target, img_path\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQHqhLtrbWF-"
      },
      "source": [
        "Se carga la base de datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqhZicltbWGA",
        "outputId": "1de050e5-2dfb-4b28-83cc-808c4e148a18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import utils\n",
        "\n",
        "# use our dataset and defined transformations\n",
        "dataset = myVOCDataset(data_dir, train=True, data_augm=False)\n",
        "dataset_test = myVOCDataset(data_dir, train=False, data_augm=False)\n",
        "\n",
        "# define training and test data loaders\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, # to debug, fix num_workers=0\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test, batch_size=batch_size, shuffle=False, num_workers=num_workers,\n",
        "    collate_fn=utils.collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Nf7S3EfbWGD"
      },
      "source": [
        "### Medidas de evaluación\n",
        "\n",
        "Asimismo, se define la función que testea la bondad de nuestro modelo en términos de detección (precisión y recall) y términos de clasificación (de entre los objetos detectados, cuáles se clasifican correctamente de acuerdo a su categoría).\n",
        "La función recibe como parámetros:\n",
        "- __model__: la CNN que evaluar.\n",
        "- __dataloader__: el cargador de los datos de test.\n",
        "- __class_names__: nombres de las clases de objetos a detectar (siempre hay que incluir en primer lugar la clase *background*).\n",
        "- __th_score__: la red proporciona cada objeto detectado con una probabilidad entre 0 y 1. Este umbral descarta los objetos cuya probabilidad<th_score, es decir, es una decisión sobre el punto de operación del sistema.\n",
        "- __th_iou__: impone un umbral mínimo de IoU a partir del cual considerar una detección como correcta, es decir, es una decisión sobre la manera de evaluar el sistema.\n",
        "- __result_dir__: ruta al directorio de resultados.\n",
        "- __SAVE_OPT__: para guardar o no los resultados de test como imágenes con el *ground truth*, las *bounding boxes* predichas y los *anchors* correspondientes.\n",
        "- __SAVE_FULL__: para guardar los resultados totales sobre cada imagen (True) o guardarlos en carpetas diferentes para cada tamaño de *anchor* para su posterior análisis (False).\n",
        "- __VERBOSE__: imprime por pantalla las medidas de evaluación totales (sin tener en cuenta cada clase) si es False, o las totales y las marginales para cada clase si está a True.\n",
        "- __batchsize__: debe ser igual a 1.\n",
        "\n",
        "La función proporciona como salidas:\n",
        "- __precision_RPN__: la precisión para la detección de objetos de la RPN.\n",
        "- __recall_RPN__: el *recall* para la detección de objetos de la RPN.\n",
        "- __f1_score_RPN__: el F1 score para la detección, que tomamos como la medida principal del rendimiento del sistema.\n",
        "- __cm_global__: matriz de confusión entre clases para la clasificación.\n",
        "- __prec_rec_global__: precisión y *recall* para la clasificación en general.\n",
        "- __prec_rec_marginal__: precisión y *recall* para la clasificación de cada clase en particular.\n",
        "- __total_anchors__: todos los anchors de la red para el conjunto de test, para su posterior análisis.\n",
        "- __total_labels__: etiquetas de dichos *anchors*.\n",
        "\n",
        "__**IMPORTANTE:__ cabe destacar que la función de inferencia devuelve las medidas de evaluación tanto para la RPN (`precision_RPN`, `recall_RPN` y `f1_score_RPN`): es decir, estas medidas se refieren al *objectness* o capacidad de detección de objetos de la red sin tener en cuenta su categoría; como para la clasificación final de la red (`cm_global`, `prec_rec_global` y `prec_rec_marginal`): estas medidas se refieren a cómo de eficiente es la red clasificando las salidas de la RPN en función de su categoría."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def anchors(dataloader):\n",
        "    all_boxes = []\n",
        "    for imgs, targets, _ in dataloader:\n",
        "        boxes = targets[0][\"boxes\"].cpu().numpy()\n",
        "        for xmin, ymin, xmax, ymax in boxes:\n",
        "            w, h = xmax - xmin, ymax - ymin\n",
        "            all_boxes.append([w, h])\n",
        "\n",
        "    X = np.array(all_boxes)\n",
        "    k = 5\n",
        "    kmeans = KMeans(n_clusters=k, random_state=0).fit(X)\n",
        "    centers = kmeans.cluster_centers_\n",
        "    anchor_sizes  = [float(round(np.sqrt(w*h),1)) for w,h in centers]\n",
        "    aspect_ratios = [float(round(w/h,2)) for w,h in centers]\n",
        "\n",
        "    print(\"Anchor sizes:\", anchor_sizes)\n",
        "    print(\"Aspect ratios:\", aspect_ratios)\n",
        "    size_anchors = anchor_sizes\n",
        "\n",
        "    anchor_sizes  = [float(round(np.sqrt(w*h),1)) for w,h in centers]\n",
        "    aspect_ratios = [float(round(w/h,2))          for w,h in centers]\n",
        "    return (size_anchors, aspect_ratios)"
      ],
      "metadata": {
        "id": "y9DsoN0Ip3Hc"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "jFgZZM3TbWGE"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "import os\n",
        "import cv2\n",
        "from torchvision.transforms import functional as F\n",
        "from external import bb_intersection_over_union\n",
        "\n",
        "def test_detection_model(model, dataloader, class_names, th_score, th_iou, result_dir, SAVE_OPT, SAVE_FULL, VERBOSE, batch_size=1):\n",
        "    since = time.time()\n",
        "    size_anchors, aspect_ratios = anchors(dataloader)\n",
        "    bins = [48,96,192,384]\n",
        "\n",
        "    min_size = 800\n",
        "    max_size = 1333\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    ret = 0\n",
        "    rel = np.zeros((len(class_names)-1,), dtype=int)\n",
        "    ret_rel = np.zeros((len(class_names)-1), dtype=int)\n",
        "\n",
        "    y_true = np.zeros((0,), dtype='int')\n",
        "    y_pred = np.zeros((0,), dtype='int')\n",
        "    y_score = np.zeros((0,), dtype='int')\n",
        "    batch_counter = 0\n",
        "    total_anchors = np.zeros((0,4), dtype='float32')\n",
        "    total_labels = np.zeros((0,), dtype=int)\n",
        "\n",
        "    print('Evaluating...')\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets, paths in dataloader:\n",
        "            batch_counter += 1\n",
        "            inputs = list(image.to(device) for image in inputs)\n",
        "            labels = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            size_img = inputs[0].size()[1:]\n",
        "            scale_factor = min_size / min(size_img)\n",
        "            if max(size_img) * scale_factor > max_size:\n",
        "                scale_factor = max_size / max(size_img)\n",
        "\n",
        "            if (np.array(labels[0]['boxes'].cpu()).shape[0] == 0):\n",
        "                gt_boxes = np.zeros((0,4), dtype='float32')\n",
        "            else:\n",
        "                gt_boxes = np.array(labels[0]['boxes'].detach().cpu())\n",
        "                gt_labels = np.array(labels[0]['labels'].detach().cpu())\n",
        "                for j in range(1, len(class_names)):\n",
        "                    rel[j-1] += np.sum(gt_labels == j)\n",
        "\n",
        "            pred = model(inputs)\n",
        "            pred[0]['scores'] = pred[0]['scores'].detach().cpu()\n",
        "            pred[0]['boxes'] = pred[0]['boxes'].detach().cpu()\n",
        "            pred[0]['labels'] = pred[0]['labels'].detach().cpu()\n",
        "            pred[0]['anchors'] = pred[0]['anchors'].detach().cpu()\n",
        "\n",
        "            if (len(pred[0]['scores'].numpy()) == 0):\n",
        "                pred_boxes = np.zeros((0,4), dtype='float32')\n",
        "                pred_labels = np.zeros((0,), dtype=int)\n",
        "            else:\n",
        "                pred_score = list(pred[0]['scores'].numpy())\n",
        "                if (pred_score[0] > th_score):\n",
        "                    pred_t = [pred_score.index(x) for x in pred_score if x > th_score][-1]\n",
        "                    pred_class = [class_names[i] for i in list(pred[0]['labels'].numpy())]\n",
        "                    pred_labels = pred[0]['labels'].numpy()\n",
        "                    pred_boxes = np.array([[i[0], i[1], i[2], i[3]] for i in list(pred[0]['boxes'].numpy())][:pred_t+1])\n",
        "                    pred_anchors = np.array([[i[0], i[1], i[2], i[3]] for i in list(pred[0]['anchors'].numpy())][:pred_t+1])\n",
        "                    total_anchors = np.concatenate((total_anchors, pred_anchors), axis=0)\n",
        "                    pred_class = pred_class[:pred_t+1]\n",
        "                    pred_labels = pred_labels[:pred_t+1]\n",
        "                    total_labels = np.concatenate((total_labels, pred_labels), axis=0)\n",
        "                else:\n",
        "                    pred_boxes = np.zeros((0,4), dtype='float32')\n",
        "                    pred_labels = np.zeros((0,), dtype=int)\n",
        "\n",
        "            ret += len(pred_labels)\n",
        "\n",
        "            for j in range(gt_boxes.shape[0]):\n",
        "                ious = np.zeros((pred_boxes.shape[0],), dtype='float')\n",
        "                for k in range(pred_boxes.shape[0]):\n",
        "                    ious[k] = bb_intersection_over_union(gt_boxes[j,:], pred_boxes[k,:])\n",
        "                if (len(ious) > 0):\n",
        "                    iou_max = np.max(ious)\n",
        "                    pos = np.argmax(ious)\n",
        "                    if (iou_max > th_iou):\n",
        "                        ret_rel[gt_labels[j]-1] += 1\n",
        "                        y_true = np.concatenate((y_true, gt_labels[j][np.newaxis]), axis=0)\n",
        "                        y_pred = np.concatenate((y_pred, pred_labels[pos][np.newaxis]), axis=0)\n",
        "\n",
        "            # Visualizations\n",
        "            if SAVE_OPT and SAVE_FULL:\n",
        "                aux = paths[0].split('/')\n",
        "                folder_path = os.path.join(result_dir, 'instances_pred_full')\n",
        "                os.makedirs(folder_path, exist_ok=True)\n",
        "                img = np.array(F.to_pil_image(inputs[0].cpu()))\n",
        "                for i in range(pred_boxes.shape[0]):\n",
        "                    cv2.rectangle(img, (int(pred_boxes[i][0]), int(pred_boxes[i][1])), (int(pred_boxes[i][2]), int(pred_boxes[i][3])), color=(255, 0, 0), thickness=1)\n",
        "                    cv2.putText(img, pred_class[i], (int(pred_boxes[i][0]), int(pred_boxes[i][1])), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), thickness=1)\n",
        "                for i in range(gt_boxes.shape[0]):\n",
        "                    cv2.rectangle(img, (int(gt_boxes[i][0]), int(gt_boxes[i][1])), (int(gt_boxes[i][2]), int(gt_boxes[i][3])), color=(0, 255, 0), thickness=1)\n",
        "                    cv2.putText(img, class_names[gt_labels[i]], (int(gt_boxes[i][0]), int(gt_boxes[i][1])), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), thickness=1)\n",
        "                pred_path = os.path.join(folder_path, aux[-1])\n",
        "                cv2.imwrite(pred_path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "            elif SAVE_OPT and not SAVE_FULL:\n",
        "                aux = paths[0].split('/')\n",
        "                folder_path = os.path.join(result_dir, 'instances_pred')\n",
        "                os.makedirs(folder_path, exist_ok=True)\n",
        "                for i in range(pred_boxes.shape[0]):\n",
        "                    anchor_size = size_anchors[np.digitize(np.sqrt((pred_anchors[i,2] - pred_anchors[i,0]) * (pred_anchors[i,3] - pred_anchors[i,1])), bins)]\n",
        "                    img = np.array(F.to_pil_image(inputs[0].cpu()))\n",
        "                    cv2.rectangle(img, (int(pred_boxes[i][0]), int(pred_boxes[i][1])), (int(pred_boxes[i][2]), int(pred_boxes[i][3])), color=(255, 0, 0), thickness=1)\n",
        "                    cv2.rectangle(img, (int(pred_anchors[i][0]/scale_factor), int(pred_anchors[i][1]/scale_factor)), (int(pred_anchors[i][2]/scale_factor), int(pred_anchors[i][3]/scale_factor)), color=(0, 0, 255), thickness=1)\n",
        "                    cv2.putText(img, pred_class[i], (int(pred_boxes[i][0]), int(pred_boxes[i][1])), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), thickness=1)\n",
        "                    for j in range(gt_boxes.shape[0]):\n",
        "                        cv2.rectangle(img, (int(gt_boxes[j][0]), int(gt_boxes[j][1])), (int(gt_boxes[j][2]), int(gt_boxes[j][3])), color=(0, 255, 0), thickness=1)\n",
        "                        cv2.putText(img, class_names[gt_labels[j]], (int(gt_boxes[j][0]), int(gt_boxes[j][1])), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), thickness=1)\n",
        "                    anchor_folder = os.path.join(folder_path, str(anchor_size))\n",
        "                    os.makedirs(anchor_folder, exist_ok=True)\n",
        "                    pred_path = os.path.join(anchor_folder, aux[-1][:-4] + '_' + str(i) + '.png')\n",
        "                    cv2.imwrite(pred_path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    # Metrics\n",
        "    precision_RPN = np.sum(ret_rel) / ret if ret > 0 else 0\n",
        "    recall_RPN = np.zeros((rel.size,), dtype='float32')\n",
        "    for j in range(rel.size):\n",
        "        recall_RPN[j] = ret_rel[j] / rel[j] if rel[j] > 0 else 0\n",
        "\n",
        "    f1_score_RPN = 2 * np.mean(recall_RPN) * precision_RPN / (np.mean(recall_RPN) + precision_RPN) if (np.mean(recall_RPN) > 0 and precision_RPN > 0) else 0\n",
        "\n",
        "    prec_rec_marginal = precision_recall_fscore_support(y_true, y_pred, average=None, labels=[f for f in range(1, len(class_names))], zero_division=0)\n",
        "    prec_rec_global = precision_recall_fscore_support(y_true, y_pred, average='macro', labels=[f for f in range(1, len(class_names))], zero_division=0)\n",
        "    cm_global = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Evaluation complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Objectness-RPN. F1: {:4f}.     Precision: {:4f}. Recall: {:4f}'.format(f1_score_RPN, precision_RPN, np.mean(recall_RPN)))\n",
        "    if VERBOSE:\n",
        "        for i in range(1, len(class_names)):\n",
        "            print('Class: ' + class_names[i] + '. Recall: {:1d}/{:1d}'.format(ret_rel[i-1], rel[i-1]))\n",
        "        print('')\n",
        "\n",
        "    if (prec_rec_global[0] + prec_rec_global[1]) > 0:\n",
        "        f1_class = 2 * prec_rec_global[0] * prec_rec_global[1] / (prec_rec_global[0] + prec_rec_global[1])\n",
        "    else:\n",
        "        f1_class = 0\n",
        "    print('Global classification: F1: {:4f}.     Precision: {:4f}. Recall: {:4f}'.format(f1_class, prec_rec_global[0], prec_rec_global[1]))\n",
        "\n",
        "    if VERBOSE:\n",
        "        for i in range(1, len(class_names)):\n",
        "            f1_class = 2 * prec_rec_marginal[0][i-1] * prec_rec_marginal[1][i-1] / (prec_rec_marginal[0][i-1] + prec_rec_marginal[1][i-1]) if (prec_rec_marginal[0][i-1] + prec_rec_marginal[1][i-1]) > 0 else 0\n",
        "            print('Class: ' + class_names[i] + '. F1: {:4f}.     Precision: {:4f}. Recall: {:4f}'.format(f1_class, prec_rec_marginal[0][i-1], prec_rec_marginal[1][i-1]))\n",
        "\n",
        "    return (precision_RPN, recall_RPN, f1_score_RPN, cm_global, prec_rec_global, prec_rec_marginal, total_anchors, total_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZpCkDLRbWGG"
      },
      "source": [
        "### Entrenamiento\n",
        "\n",
        "El entrenamiento de la red se realiza durante 24 epochs, reduciendo la tasa de entrenamiento a medida que se avanza en el entrenamiento. El código produce un archivo denominado _log.csv_ donde se puede analizar la variación de las funciones de pérdida en cada _epoch_ de entrenamiento, así como la correspondiente precisión y recall en el conjunto de test tanto para detección como para clasificación. Compruebe que las funciones de pérdida disminuyen  de manera aproximadamente monótona, y que las medidas de evaluación en test se estancan a medida que avanza el entrenamiento (y son más ruidosas).\n",
        "\n",
        "__**IMPORTANTE:__ se considera el mejor _epoch_ de la red como aquel que proporciona la mejor medida F1-score (media armónica de la precisión y el *recall*) y se almacenan sus pesos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQl1bKkzbWGI",
        "outputId": "e6e8d5fb-536d-4c83-e093-be8ab87cc36c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anchor sizes: [50.0, 216.0, 258.70001220703125, 324.29998779296875, 136.8000030517578]\n",
            "Aspect ratios: [0.6600000262260437, 1.4700000286102295, 0.41999998688697815, 1.8600000143051147, 0.6399999856948853]\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "from engine import train_one_epoch, eval_one_epoch\n",
        "import time\n",
        "\n",
        "size_anchors, aspect_ratios = anchors(data_loader)\n",
        "\n",
        "\n",
        "\n",
        "# ───> AQUÍ creas tu AnchorGenerator con los K anchors por nivel\n",
        "num_fpn_levels = 5\n",
        "sizes_per_level  = [tuple(size_anchors)]  * num_fpn_levels\n",
        "ratios_per_level = [tuple(aspect_ratios)] * num_fpn_levels\n",
        "\n",
        "anchor_generator = AnchorGenerator(\n",
        "    sizes=sizes_per_level,\n",
        "    aspect_ratios=ratios_per_level\n",
        ")\n",
        "\n",
        "# ───> Y a continuación (re)creas tu modelo usando ese anchor_generator\n",
        "backbone = resnet_fpn_backbone('resnet50', pretrained=True)\n",
        "model = FasterRCNN(\n",
        "    backbone,\n",
        "    num_classes=len(class_names),\n",
        "    rpn_anchor_generator=anchor_generator\n",
        ")\n",
        "# We create the baseline folder\n",
        "result_dir='baseline_results'\n",
        "if not os.path.exists(result_dir):\n",
        "    os.makedirs(result_dir)\n",
        "\n",
        "# Weights for L_objectness, L_reg, L_cls y L_reg2\n",
        "weights=[1,1,1,1]\n",
        "\n",
        "# Training\n",
        "# construct an optimizer\n",
        "params = [p for p in model_ft.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=lr,\n",
        "                                momentum=0.9, weight_decay=0.0005)\n",
        "# and a learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                                step_size=step_size,\n",
        "                                                gamma=0.1)\n",
        "\n",
        "# CSV file for training results\n",
        "if os.path.exists(os.path.join(result_dir,'log.csv')):\n",
        "    os.remove(os.path.join(result_dir,'log.csv'))\n",
        "csv_file=open(os.path.join(os.path.join(result_dir,'log.csv')),'w')\n",
        "coord_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "coord_writer.writerow(['Train total','Train rpn_box_reg','Train objectness','Train box_reg','Train classifier',\n",
        "                        'Val total','Val rpn_box_reg','Val objectness','Val box_reg','Val classifier'])\n",
        "best_f1=0\n",
        "for epoch in range(num_epochs):\n",
        "    if not os.path.exists(os.path.join(result_dir,'faster-rcnn-epoch{}.pth'.format(epoch))):\n",
        "        #     train for one epoch, printing every epoch\n",
        "        train_aux_losses=train_one_epoch(model_ft, optimizer, data_loader, device, weights, epoch, print_freq=250)\n",
        "        # Update the learning rate\n",
        "        lr_scheduler.step()\n",
        "        # Validation\n",
        "        val_aux_losses=eval_one_epoch(model_ft, data_loader_test, device, epoch, print_freq=250)\n",
        "        # Save the losses\n",
        "        coord_writer.writerow([str(train_aux_losses['total']),str(train_aux_losses['rpn_box_reg']),str(train_aux_losses['objectness']),\n",
        "                                str(train_aux_losses['box_reg']),str(train_aux_losses['classifier']),\n",
        "                                str(val_aux_losses['total']),str(val_aux_losses['rpn_box_reg']),str(val_aux_losses['objectness']),\n",
        "                                str(val_aux_losses['box_reg']),str(val_aux_losses['classifier'])])\n",
        "        # Evaluation\n",
        "        [precision,recall,f1_score,cm,total,partial,_,_]=test_detection_model(model_ft, data_loader_test, class_names, 0.5,0.5, result_dir, False, False, False)\n",
        "        # Save the state and the model with best AP-score for inference\n",
        "        state = {'epoch': epoch + 1, 'state_dict': model_ft.state_dict(),\n",
        "                  'optimizer': optimizer.state_dict(),\n",
        "                  'scheduler':lr_scheduler.state_dict(),}\n",
        "        torch.save(state, os.path.join(result_dir,'faster-rcnn-epoch{}.pth'.format(epoch)))\n",
        "        if (f1_score>best_f1):\n",
        "            torch.save(state, os.path.join(result_dir,'best_model.pth'.format(epoch)))\n",
        "            best_f1=f1_score\n",
        "    \"\"\"else:\n",
        "        # Load this epoch information to resume training\n",
        "        print(\"=> loading checkpoint '{}'\".format(epoch))\n",
        "        checkpoint = torch.load(os.path.join(result_dir,'faster-rcnn-epoch{}.pth'.format(epoch)),map_location='cpu')\n",
        "        model_ft.load_state_dict(checkpoint['state_dict'])\n",
        "        lr_scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "        model_ft.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        print(\"=> loaded checkpoint '{}'\" .format(epoch))\"\"\"\n",
        "\n",
        "csv_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0P-Dx8U-bWGL"
      },
      "source": [
        "### Evaluación\n",
        "\n",
        "Tras entrenar la red, se van a obtener los resultados para el conjunto de test. En primer lugar, se cargan los pesos de la red entrenada en el modelo y se llama a la función de evaluación. Preste atención a los parámetros que recibe la función.\n",
        "\n",
        "Analice la información que devuelve la función. Note la diferencia entre las medidas de *objectness*, sobre la RPN; y las medidas finales de clasificación de objetos detectados del sistema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pPgRCnMbWGM",
        "outputId": "b9f58720-0cc2-4f96-d1b4-a1ba85060081"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anchor sizes: [308.8999938964844, 50.20000076293945, 134.10000610351562, 210.1999969482422, 254.39999389648438]\n",
            "Aspect ratios: [1.809999942779541, 0.6600000262260437, 0.8299999833106995, 0.2800000011920929, 0.9700000286102295]\n",
            "Evaluating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation complete in 10m 30s\n",
            "Objectness-RPN. F1: 0.695509.     Precision: 0.646259. Recall: 0.752885\n",
            "Class: bottle. Recall: 22/36\n",
            "Class: chair. Recall: 37/52\n",
            "Class: dining_table. Recall: 12/15\n",
            "Class: sofa. Recall: 24/27\n",
            "\n",
            "Global classification: F1: 0.921538.     Precision: 0.940687. Recall: 0.903153\n",
            "Class: bottle. F1: 1.000000.     Precision: 1.000000. Recall: 1.000000\n",
            "Class: chair. F1: 0.897436.     Precision: 0.853659. Recall: 0.945946\n",
            "Class: dining_table. F1: 0.909091.     Precision: 1.000000. Recall: 0.833333\n",
            "Class: sofa. F1: 0.869565.     Precision: 0.909091. Recall: 0.833333\n"
          ]
        }
      ],
      "source": [
        "# Inference\n",
        "model_weights = torch.load(os.path.join(result_dir,'best_model.pth'.format(num_epochs-1)),map_location=device)['state_dict']\n",
        "model_ft.load_state_dict(model_weights)\n",
        "_=test_detection_model(model_ft, data_loader_test, class_names, 0.5, 0.5, result_dir, True, False, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJ6thL68bWGQ"
      },
      "source": [
        "## Parte 3. Experimentos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRDMaE1qbWGQ"
      },
      "source": [
        "### 1. Influencia de los umbrales en la inferencia.\n",
        "\n",
        "#### Elección del punto de operación del sistema\n",
        "\n",
        "Analice la influencia del umbral `th_score` sobre los resultados en inferencia del modelo con las medidas de detección y clasificación globales; marginales para cada clase (ponga `VERBOSE` a `True` si así lo desea); y visualmente si lo desea (ponga `SAVE_OPT` a `True`). Para ello, varíe el umbral entre 0.3, 0.5 y 0.7."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNPBbfM9bWGS"
      },
      "outputs": [],
      "source": [
        "# Include your code here\n",
        "print('TH_SCORE=0.3')\n",
        "_=test_detection_model(model_ft, data_loader_test, class_names, 0.3, 0.5, result_dir, False, False, False)\n",
        "print('\\nTH_SCORE=0.5')\n",
        "_=test_detection_model(model_ft, data_loader_test, class_names, 0.5, 0.5, result_dir, False, False, False)\n",
        "print('\\nTH_SCORE=0.7')\n",
        "_=test_detection_model(model_ft, data_loader_test, class_names, 0.7, 0.5, result_dir, False, False, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE05lYipbWGV"
      },
      "source": [
        "#### Elección del punto de operación del sistema\n",
        "\n",
        "Analice la influencia del umbral `th_iou` sobre los resultados en inferencia del modelo, con las medidas de detección y clasificación globales; marginales para cada clase (ponga `VERBOSE` a `True` si así lo desea); y visualmente si lo desea (ponga `SAVE_OPT` a `True`). Para ello, varíe el umbral entre 0.3, 0.5 y 0.7."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mr2hU0qmbWGV"
      },
      "outputs": [],
      "source": [
        "# Include your code here\n",
        "print('TH_IOU=0.3')\n",
        "_=test_detection_model(model_ft, data_loader_test, class_names, 0.5, 0.3, result_dir, False, False, False)\n",
        "print('\\nTH_IOU=0.5')\n",
        "_=test_detection_model(model_ft, data_loader_test, class_names, 0.5, 0.5, result_dir, False, False, False)\n",
        "print('\\nTH_IOU=0.7')\n",
        "_=test_detection_model(model_ft, data_loader_test, class_names, 0.5, 0.7, result_dir, False, False, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmlWE9gBbWGY"
      },
      "source": [
        "### 2. Análisis del tamaño de los _anchors_ .\n",
        "\n",
        "Las imágenes de la base de datos PASCAL VOC tienen un tamaño máximo de 500 píxeles en su lado mayor y un lado menor de tamaño variable, y por tanto, distintas relaciones de aspecto. Hay que tener en cuenta que Pytorch escala las imágenes para que su lado mínimo mida 800 píxeles o su lado máximo 1333 (véase la función __resize__ de __torchvision_05.models.detection.transform__). En primer lugar, se calcula la distribución de tamaño y relaciones de aspecto de los objetos de la base de datos en entrenamiento y test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VrcISn3xbWGZ",
        "outputId": "a73dc921-ad7f-49d3-fb21-5553e769304c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x1500 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Include your code here\n",
        "import matplotlib.pyplot as plt\n",
        "from external import get_gt_anchors\n",
        "# define training and test data loaders\n",
        "data_loader_0 = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=batch_size, shuffle=True, num_workers=0,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "data_loader_test_0 = torch.utils.data.DataLoader(\n",
        "    dataset_test, batch_size=batch_size, shuffle=False, num_workers=0,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "# TRAIN\n",
        "ar, labels, size_anchors=get_gt_anchors(data_loader_0)\n",
        "fig, axes = plt.subplots(nrows=2, ncols=2,figsize=(15,15))\n",
        "ax0, ax1, ax2, ax3 = axes.flatten()\n",
        "ax0.hist([ar[np.where(labels==1)[0]],ar[np.where(labels==2)[0]],ar[np.where(labels==3)[0]],ar[np.where(labels==4)[0]]],color=['blue','orange','green','purple'],label=class_names[1:])\n",
        "ax0.legend(prop={'size': 10})\n",
        "ax0.set_title('Aspect ratio, train')\n",
        "ax1.hist([size_anchors[np.where(labels==1)[0]],size_anchors[np.where(labels==2)[0]],size_anchors[np.where(labels==3)[0]],size_anchors[np.where(labels==4)[0]]],color=['blue','orange','green','purple'],label=class_names[1:])\n",
        "ax1.legend(prop={'size': 10})\n",
        "ax1.set_title('Box size, train')\n",
        "\n",
        "# TEST\n",
        "ar, labels, size_anchors=get_gt_anchors(data_loader_test_0)\n",
        "ax2.hist([ar[np.where(labels==1)[0]],ar[np.where(labels==2)[0]],ar[np.where(labels==3)[0]],ar[np.where(labels==4)[0]]],color=['blue','orange','green','purple'],label=class_names[1:])\n",
        "ax2.legend(prop={'size': 10})\n",
        "ax2.set_title('Aspect ratio, test')\n",
        "ax3.hist([size_anchors[np.where(labels==1)[0]],size_anchors[np.where(labels==2)[0]],size_anchors[np.where(labels==3)[0]],size_anchors[np.where(labels==4)[0]]],color=['blue','orange','green','purple'],label=class_names[1:])\n",
        "ax3.legend(prop={'size': 10})\n",
        "ax3.set_title('Box size, test')\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ns4s3ld0bWGb"
      },
      "source": [
        "A continuación se van a analizar los _anchors_ que proporciona la red para cada objeto, para ver si la distribución corresponde a las descritas anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4e71ZhRmbWGc",
        "outputId": "31f2dbf0-347c-4cc6-a340-12f17c1d2314"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anchor sizes: [308.8999938964844, 50.20000076293945, 134.10000610351562, 210.1999969482422, 254.39999389648438]\n",
            "Aspect ratios: [1.809999942779541, 0.6600000262260437, 0.8299999833106995, 0.2800000011920929, 0.9700000286102295]\n",
            "Evaluating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation complete in 10m 21s\n",
            "Objectness-RPN. F1: 0.695509.     Precision: 0.646259. Recall: 0.752885\n",
            "Global classification: F1: 0.921538.     Precision: 0.940687. Recall: 0.903153\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Anchor size, train')"
            ]
          },
          "metadata": {},
          "execution_count": 45
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x1500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Inference: extract the selected anchors\n",
        "model_weights = torch.load(os.path.join(result_dir,'best_model.pth'.format(num_epochs-1)),map_location=device)['state_dict']\n",
        "model_ft.load_state_dict(model_weights)\n",
        "[_,_,_,_,_,_,anchors,l_t]=test_detection_model(model_ft, data_loader_test, class_names, 0.5, 0.5, result_dir, False, False, False)\n",
        "\n",
        "# Anchor statistics\n",
        "ar=[]\n",
        "labels=[]\n",
        "size_anchors=[]\n",
        "for i in range(anchors.shape[0]):\n",
        "    labels.append(l_t[i])\n",
        "    size_anchors.append(np.sqrt((anchors[i,2]-anchors[i,0])*(anchors[i,3]-anchors[i,1])))\n",
        "    ar.append((anchors[i,2]-anchors[i,0])/(anchors[i,3]-anchors[i,1]))\n",
        "\n",
        "ar=np.array(ar)\n",
        "labels=np.array(labels)\n",
        "size_anchors=np.array(size_anchors)\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,15))\n",
        "ax0, ax1 = axes.flatten()\n",
        "ax0.hist([ar[np.where(labels==1)[0]],ar[np.where(labels==2)[0]],ar[np.where(labels==3)[0]],ar[np.where(labels==4)[0]]],color=['blue','orange','green','purple'],label=class_names[1:])\n",
        "ax0.legend(prop={'size': 10})\n",
        "ax0.set_title('Aspect ratio, train')\n",
        "ax1.hist([size_anchors[np.where(labels==1)[0]],size_anchors[np.where(labels==2)[0]],size_anchors[np.where(labels==3)[0]],size_anchors[np.where(labels==4)[0]]],color=['blue','orange','green','purple'],label=class_names[1:])\n",
        "ax1.legend(prop={'size': 10})\n",
        "ax1.set_title('Anchor size, train')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnBO6klZbWGf"
      },
      "source": [
        "- ¿La distribución de los _anchors_ se corresponde con la distribución de las *bounding boxes* en entrenamiento y test?\n",
        "\n",
        "- A la vista de los resultados, ¿por qué cree que la librería cambia el tamaño de las imágenes a la entrada de la red?\n",
        "\n",
        "#### Tenga en cuenta que la regresión se mueve en el hueco entre dos escalas consecutivas (doble de tamaño).\n",
        "\n",
        "- Analice las salidas visuales de la red, los _anchors_ y la regresión. ¿Es la regresión buena en todos los casos? ¿En qué tipo de objetos es más útil, más grandes o más pequeños? ¿Por qué?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P51uzshNbWGg"
      },
      "source": [
        "### 3. _RoI pooling_ y clasificación.\n",
        "\n",
        "En este apartado, se va a obtener una representación visual de las salidas del *RoI pooling* para las distintas clases (a partir de la función externa `visualize_roipooling`), y se van a analizar los resultados de clasificación.\n",
        "\n",
        "En la siguiente celda de código se extraen representaciones visuales del *RoI pooling* para cada objeto detectado en la carpeta`roipool`, dentro de la carpeta de resultados. El *RoI pooling* proporciona como salida un mapa de características $F_r$ de tamaño $HxWxC=7x7x2048$. Cada representación, por su parte, corresponde a una celda de 7x7 en la que se representan el valor máximo de este mapa $F_r$ sobre todos los canales (es decir, cada elemento contiene el valor máximo de todos los canales para cada celda). De este modo se puede inferir la distribución espacial de las activaciones en el *RoI pooling*.  Ejecute la siguiente celda y analice las representaciones de salida.\n",
        "\n",
        "__**IMPORTANTE__: no se utiliza la media para extraer las representaciones (*a priori* puede parecer mejor) ya que en las redes convolucionales la mayoría de los valores de los canales en cierta posición suelen ser muy bajos, y esto hace que los valores medios sean muy similares.\n",
        "\n",
        "__**IMPORTANTE:__ la función `visualize_roipooling` hace uso de los *hook* de Pytorch, que se utilizarán en el siguiente apartado. Los *hook* de Pytorch son una serie de funciones que permiten modificar los datos de entrada o salida de alguna capa de la red durante la ejecución de la red. Esto permite tener acceso a datos intermedios de la red en tiempo de ejecución, así como realizar un *debug* controlado de la red neuronal. En concreto se usa un *register_forward_hook()*, se ejecuta tras el método *forward* de cualquier capa de la red y tiene acceso a sus entradas y salidas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9ivq3FfbWGi"
      },
      "outputs": [],
      "source": [
        "from external import visualize_roipooling\n",
        "\n",
        "# For the roi-pooling experiment, analyze the top bounding-boxes (th_score>0.7) and only those with\n",
        "# th_iou>0.7 with the ground-truth\n",
        "visualize_roipooling(model_ft, data_loader_test, device, class_names, 0.7, 0.7, result_dir, True)\n",
        "# For the confusion matrix, experiment with the standard thresholds\n",
        "[precision,recall,ap_score,cm,_,_,_,_]=test_detection_model(model_ft, data_loader_test, class_names, 0.5, 0.5, result_dir, True, False, True)\n",
        "dconf=ConfusionMatrixDisplay(cm,display_labels=class_names[1:])\n",
        "dconf.plot()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGZUPDHFbWGl"
      },
      "source": [
        "- Céntrese en el caso de los objetos que aparecen al completo. ¿Se puede inferir la clase del objeto de la distribución espacial del *RoI pooling*, independientemente de su escala? Muestre algún ejemplo. Justifique la utilidad del *RoI pooling*.\n",
        "\n",
        "- Analice la matriz de confusión. ¿Qué clases es más sencillo confundir entre sí? ¿Por qué?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV0BxL1pbWGl"
      },
      "source": [
        "### 4. Funciones de pérdida.\n",
        "\n",
        "Analice qué ocurre cuando se actúa sobre las funciones de pérdida en la red. Para ello, realice los siguientes experimentos:\n",
        "\n",
        "- Elimine la regresión de *bounding boxes* de la red. Para ello se ponen las salidas de la regresión a 0 (vea cómo se realiza esto a partir de los *hooks*) y se anulan las losses de regresión (con el parámetro `weights`). Esto es más sencillo que modificar la arquitectura de red y el código para eliminar dichas capas. Analice las salidas visuales y los resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpB-IUYCbWGl"
      },
      "outputs": [],
      "source": [
        "def zero_features_hook(self, input, output):\n",
        "    output=torch.zeros_like(output)\n",
        "    return output\n",
        "\n",
        "def zero_features_tuple_hook(self, input, output):\n",
        "    output=(output[0],torch.zeros_like(output[1]))\n",
        "    return output\n",
        "\n",
        "def get_model_detection_no_bbox_regression(num_classes):\n",
        "    # load a Faster RCNN model pre-trained on COCO\n",
        "    model = torchvision_05.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "    # get number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    # Regression outputs to 0\n",
        "    model.rpn.head.bbox_pred.register_forward_hook(zero_features_hook)\n",
        "    model.roi_heads.box_predictor.register_forward_hook(zero_features_tuple_hook)\n",
        "\n",
        "    return model\n",
        "\n",
        "model_ft = get_model_detection_no_bbox_regression(num_classes)\n",
        "# Send the model to GPU\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# We create the no_regression folder\n",
        "result_dir='no_regression'\n",
        "if not os.path.exists(result_dir):\n",
        "    os.makedirs(result_dir)\n",
        "\n",
        "# Weights for L_objectness, L_reg, L_cls y L_reg2\n",
        "weights=[1,0,1,0]\n",
        "\n",
        "# Training\n",
        "# construct an optimizer\n",
        "params = [p for p in model_ft.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=lr,\n",
        "                                momentum=0.9, weight_decay=0.0005)\n",
        "# and a learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                                step_size=step_size,\n",
        "                                                gamma=0.1)\n",
        "\n",
        "# CSV file for training results\n",
        "if os.path.exists(os.path.join(result_dir,'log.csv')):\n",
        "    os.remove(os.path.join(result_dir,'log.csv'))\n",
        "csv_file=open(os.path.join(os.path.join(result_dir,'log.csv')),'w')\n",
        "coord_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "coord_writer.writerow(['Train total','Train rpn_box_reg','Train objectness','Train box_reg','Train classifier',\n",
        "                        'Val total','Val rpn_box_reg','Val objectness','Val box_reg','Val classifier'])\n",
        "best_f1=0\n",
        "for epoch in range(num_epochs):\n",
        "    if not os.path.exists(os.path.join(result_dir,'faster-rcnn-epoch{}.pth'.format(epoch))):\n",
        "        #     train for one epoch, printing every epoch\n",
        "        train_aux_losses=train_one_epoch(model_ft, optimizer, data_loader, device, weights, epoch, print_freq=250)\n",
        "        # Update the learning rate\n",
        "        lr_scheduler.step()\n",
        "        # Validation\n",
        "        val_aux_losses=eval_one_epoch(model_ft, data_loader_test, device, epoch, print_freq=250)\n",
        "        # Update measurement file\n",
        "        coord_writer.writerow([str(train_aux_losses['total']),str(train_aux_losses['rpn_box_reg']),str(train_aux_losses['objectness']),\n",
        "                                str(train_aux_losses['box_reg']),str(train_aux_losses['classifier']),\n",
        "                                str(val_aux_losses['total']),str(val_aux_losses['rpn_box_reg']),str(val_aux_losses['objectness']),\n",
        "                                str(val_aux_losses['box_reg']),str(val_aux_losses['classifier'])])\n",
        "        # Evaluation\n",
        "        [precision,recall,f1_score,cm,total,partial,_,_]=test_detection_model(model_ft, data_loader_test, class_names, 0.5, 0.5, result_dir, False, False, False)\n",
        "        # Save the state and the model with best AP-score for inference\n",
        "        state = {'epoch': epoch + 1, 'state_dict': model_ft.state_dict(),\n",
        "                  'optimizer': optimizer.state_dict(),\n",
        "                  'scheduler':lr_scheduler.state_dict(), }\n",
        "        torch.save(state, os.path.join(result_dir,'faster-rcnn-epoch{}.pth'.format(epoch)))\n",
        "        if (f1_score>best_f1):\n",
        "            torch.save(state, os.path.join(result_dir,'best_model.pth'.format(epoch)))\n",
        "            best_f1=f1_score\n",
        "    else:\n",
        "        # Load this epoch information to resume training\n",
        "        print(\"=> loading checkpoint '{}'\".format(epoch))\n",
        "        checkpoint = torch.load(os.path.join(result_dir,'faster-rcnn-epoch{}.pth'.format(epoch)))\n",
        "        lr_scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "        model_ft.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        print(\"=> loaded checkpoint '{}'\" .format(epoch))\n",
        "\n",
        "csv_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VPanviTbWGp"
      },
      "outputs": [],
      "source": [
        "# Inference\n",
        "model_weights = torch.load(os.path.join(result_dir,'best_model.pth'.format(num_epochs-1)))['state_dict']\n",
        "model_ft.load_state_dict(model_weights)\n",
        "_=test_detection_model(model_ft, data_loader_test, class_names, 0.5, 0.5, result_dir, True, False, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xt1LFmAMbWGs"
      },
      "source": [
        "#### En las salidas de la red se puede ver que ahora las *bounding boxes* predichas coinciden con los *anchors* (las pequeñas imprecisiones se deben al redondeo en la conversión entre los tamaños de imagen externa, ~500 píxeles, e interna ~800 píxeles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jGmxgMobWGt"
      },
      "source": [
        "- Asimismo, mediante el argumento `weights` de la función `train_one_epoch` puede introducir distintos pesos a las funciones de pérdida para dar más importancia a alguna parte de la red respecto a las demás. Puede realizar algún experimento y analizar qué ocurre."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvMOM0u-bWGt"
      },
      "outputs": [],
      "source": [
        "# Include your code here\n",
        "# Weights for L_objectness, L_reg, L_cls y L_reg2\n",
        "# weights=[1,1,1,1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znxT4h71bWGw"
      },
      "source": [
        "### 5. Evaluación del trabajo autónomo del alumno.\n",
        "\n",
        "#### Criterios de evaluación\n",
        "\n",
        "De esta proyecto (si elegido) surge la segunda evaluación para la asignatura. Una vez comprendidos los fundamentos de la detección automática de objetos con redes neuronales, puede realizar los experimentos que considere oportunos. Estos experimentos pueden ir dirigidos a:\n",
        "\n",
        "- Hacer más eficiente la selección de _anchors_.\n",
        "- Analizar los resultados y proporcionar estrategias de mejora sobre la arquitectura o el procedimiento de entrenamiento.\n",
        "- Aplicar estrategias de segmentación sobre este problema, o aplicar la detección a otro problema sobre el que tenga *bounding boxes*.\n",
        "\n",
        "#### Entregables\n",
        "\n",
        "- Presentación (Fecha indicada en la entrega del proyecto en Aula Global). Este día cada grupo de alumnos tendrá un turno de 5 minutos de preguntas (máximo 5 minutos de presentación) sobre el apartado de trabajo autónomo con ayuda de un máximo de 3 transparencias.\n",
        "- Informe + Código. Los alumnos entregarán un breve informe (2 caras para la descripción, 1 cara de referencias y figuras si fuese necesaria) donde describirán los aspectos más importantes de la solución propuesta. El objetivo es que el alumno describa los análisis y extensiones que ha planteado al modelo y justifique su objetivo y utilidad de manera breve. Asimismo, se proporcionará el código utilizado para los experimentos (bien sobre este mismo Notebook, en formato `.ipynb` o bien en código Python, en formato `.py`).\n",
        "\n",
        "La fecha límite de entrega del fichero de código y el informe es la fecha indicada en la entrega del proyecto en Aula Global.\n",
        "\n",
        "#### Sugerencias\n",
        "\n",
        "A continuación se proporcionan algunas sugerencias para que el alumno trabaje de manera autónoma, a título informativo. Si lo desea, puede centrarse en implementar una o varias de ellas.\n",
        "\n",
        "- Se puede trabajar desde el punto de vista de los _anchors_ o el tamaño de la imagen, haciéndolos más eficientes. Haga un _clustering_ de los tamaños y relaciones de aspecto de la base de datos y utilice dichos anchors.\n",
        "\n",
        "- De manera análoga a lo anterior, efectuando un procedimiento adecuado de _data augmentation_ se pueden ampliar el número de tamaños y relaciones de aspecto de la base de datos.\n",
        "\n",
        "- A partir de Faster-RCNN se desarrolló Mask-RCNN, una red que además de detectar los objetos propone una segmentación píxel a píxel de los mismos. Analice las contribuciones de la misma: *RoI align* y la rama de segmentación, y efectúe alguna prueba con la misma para esta tarea (existe una implementación disponible en Pytorch y el tutorial en el que está basado este: https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html utiliza Mask-RCNN).\n",
        "\n",
        "- Se puede aplicar esta estrategia de detección a otro problema sobre el que tenga *bounding boxes*. El problema puede contar con una sola clase o ser multi-clase, y se valorará la adaptación de los procedimientos descritos a las características propias de dicha base de datos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdbbBujIqMEv"
      },
      "source": [
        "# Detección de vehículos con Faster-RCNN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHq_db18bWGx"
      },
      "outputs": [],
      "source": [
        "#Clonamos GIT para descargar nuevas clases\n",
        "#!git clone https://github.com/EscVM/OIDv4_ToolKit.git\n",
        "#%cd OIDv4_ToolKit\n",
        "#!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yNWGEcHp10f"
      },
      "outputs": [],
      "source": [
        "# Descargamos las nuevas clases Car, Truck, Bus y Motorcycle\n",
        "#!python main.py downloader --classes Car Truck Bus Motorcycle --type_csv train --limit 500\n",
        "#!python main.py downloader --classes Car Truck Bus Motorcycle --type_csv test --limit 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30Hh7gUisKuV"
      },
      "source": [
        "## Implementación adaptada para nuestro nuevo dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from PIL import Image, ImageFile, ImageDraw\n",
        "import cv2\n",
        "import random\n",
        "# Set random seed for reproducibility\n",
        "manualSeed = 999\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "# torch uses some non-deterministic algorithms\n",
        "torch.backends.cudnn.enabled = False"
      ],
      "metadata": {
        "id": "sjAxALerdhWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directorio base de tus datos descargados de Open Images\n",
        "data_dir = \"OIDv4_ToolKit/OID/Dataset\"\n",
        "\n",
        "# Parámetros de carga y entrenamiento\n",
        "num_workers = 2        # 8 si no hay problemas de memoria; 0 para depuración\n",
        "batch_size = 1         # =1 → no necesitas redimensionar imágenes\n",
        "num_classes = 5        # 4 clases + 1 fondo\n",
        "\n",
        "# Nombres de clases con el índice correspondiente (como usa Faster R-CNN)\n",
        "class_names = ['_background_', 'Car', 'Truck', 'Bus', 'Motorcycle']\n",
        "\n",
        "# Hiperparámetros de entrenamiento\n",
        "num_epochs = 5\n",
        "step_size = num_epochs // 3    # reduce el LR cada 6 épocas\n",
        "lr = 0.005                     # tasa de aprendizaje típica para Faster R-CNN\n",
        "\n",
        "# Dispositivo (GPU o CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "rbQtprVkdapg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mantenemos la librería modificada torchvision_05"
      ],
      "metadata": {
        "id": "dc11WEMXd-R3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision_05\n",
        "from torchvision_05.models.detection.rpn import AnchorGenerator\n",
        "from torchvision_05.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN\n",
        "from torchvision_05.models.detection.backbone_utils import resnet_fpn_backbone\n",
        "import torchvision.transforms.functional as F"
      ],
      "metadata": {
        "id": "mnTQW-4JdzcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Red"
      ],
      "metadata": {
        "id": "yNDu4nDdeDVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_detection(num_classes):\n",
        "    # load a Faster RCNN model pre-trained on COCO\n",
        "    model = torchvision_05.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "    # get number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "T-LkwY7Id9li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos el modelo base sin modificar"
      ],
      "metadata": {
        "id": "NLeLfmPgewu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft = get_model_detection(num_classes)\n",
        "print(model_ft)\n",
        "# Send the model to GPU\n",
        "model_ft = model_ft.to(device)"
      ],
      "metadata": {
        "id": "U-v_2oQZeuAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base de datos"
      ],
      "metadata": {
        "id": "yHwX0bG0fA7Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adaptamos MyVOCDataset a MyOIDDataset"
      ],
      "metadata": {
        "id": "NL388EyRfbuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "class MyOIDDataset(object):\n",
        "    def _init_(self, root, train=True, data_augm=False):\n",
        "        self.root = root\n",
        "        self.train = train\n",
        "        self.data_augm = data_augm\n",
        "\n",
        "        self.class_map = {\n",
        "            'Car': 1,\n",
        "            'Truck': 2,\n",
        "            'Bus': 3,\n",
        "            'Motorcycle': 4\n",
        "        }\n",
        "\n",
        "        split = 'train' if self.train else 'test'\n",
        "        base_path = os.path.join(self.root, split)\n",
        "\n",
        "        self.imgs = []\n",
        "        self.labels = []\n",
        "\n",
        "        for class_name in self.class_map:\n",
        "            class_path = os.path.join(base_path, class_name)\n",
        "            label_path = os.path.join(class_path, 'Label')\n",
        "\n",
        "            if not os.path.exists(label_path):\n",
        "                continue\n",
        "\n",
        "            for label_file in glob.glob(os.path.join(label_path, '*.txt')):\n",
        "                file_id = os.path.splitext(os.path.basename(label_file))[0]\n",
        "                image_file = os.path.join(class_path, f\"{file_id}.jpg\")\n",
        "\n",
        "                if os.path.exists(image_file):\n",
        "                    with open(label_file, 'r') as f:\n",
        "                        lines = f.readlines()[1:]  # skip header\n",
        "                        valid_lines = [line for line in lines if len(line.strip().split()) == 5]\n",
        "                        if len(valid_lines) > 0:\n",
        "                            self.imgs.append(image_file)\n",
        "                            self.labels.append(label_file)\n",
        "\n",
        "    def _getitem_(self, idx):\n",
        "        img_path = self.imgs[idx]\n",
        "        label_path = self.labels[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "\n",
        "        with open(label_path, 'r') as f:\n",
        "            lines = f.readlines()[1:]  # skip header\n",
        "            for line in lines:\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) != 5:\n",
        "                    continue\n",
        "                class_name, xmin, ymin, xmax, ymax = parts\n",
        "                if class_name not in self.class_map:\n",
        "                    continue\n",
        "                xmin = float(xmin)\n",
        "                ymin = float(ymin)\n",
        "                xmax = float(xmax)\n",
        "                ymax = float(ymax)\n",
        "                if xmax > xmin and ymax > ymin:\n",
        "                    area = (xmax - xmin) * (ymax - ymin)\n",
        "                    if area > 1:\n",
        "                        boxes.append([xmin, ymin, xmax, ymax])\n",
        "                        labels.append(self.class_map[class_name])\n",
        "\n",
        "        if len(boxes) == 0:\n",
        "            raise ValueError(f\"[ERROR] Imagen sin cajas válidas: {img_path}\")\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        iscrowd = torch.zeros((len(boxes),), dtype=torch.int64)\n",
        "\n",
        "        target = {\n",
        "            \"boxes\": boxes,\n",
        "            \"labels\": labels,\n",
        "            \"image_id\": image_id,\n",
        "            \"area\": area,\n",
        "            \"iscrowd\": iscrowd\n",
        "        }\n",
        "\n",
        "        # Data augmentation (solo en entrenamiento)\n",
        "        if self.train and self.data_augm:\n",
        "            # Horizontal flip\n",
        "            if torch.rand(1) < 0.5:\n",
        "                img = F.hflip(img)\n",
        "                boxes[:, [0, 2]] = img.width - boxes[:, [2, 0]]\n",
        "\n",
        "            # Aumentar brillo\n",
        "            if torch.rand(1) < 0.3:\n",
        "                img = F.adjust_brightness(img, brightness_factor=1.2)\n",
        "\n",
        "            # Aumentar contraste\n",
        "            if torch.rand(1) < 0.3:\n",
        "                img = F.adjust_contrast(img, contrast_factor=1.3)\n",
        "\n",
        "        img = F.to_tensor(img)\n",
        "\n",
        "        return img, target, img_path\n",
        "\n",
        "    def _len_(self):\n",
        "        return len(self.imgs)"
      ],
      "metadata": {
        "id": "zT-I3ORyfAY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se carga la BBDD"
      ],
      "metadata": {
        "id": "0GmQiIehfqye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import utils\n",
        "\n",
        "# use our dataset and defined transformations\n",
        "dataset = MyOIDDataset(data_dir, train=True,data_augm=True)\n",
        "dataset_test = MyOIDDataset(data_dir, train=False)\n",
        "\n",
        "# define training and test data loaders\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, # to debug, fix num_workers=0\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test, batch_size=batch_size, shuffle=False, num_workers=num_workers,\n",
        "    collate_fn=utils.collate_fn)"
      ],
      "metadata": {
        "id": "MxJylbmqfskF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluación"
      ],
      "metadata": {
        "id": "nwq78q1hg5et"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mantenemos la misma función de evaluación definida anteriormente para nuestro nuevo dataset"
      ],
      "metadata": {
        "id": "ns2Wd0Ztg86w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, ConfusionMatrixDisplay, average_precision_score\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "# Auxiliary function to compute the intersection over union between 2 bounding boxes\n",
        "from external import bb_intersection_over_union\n",
        "\n",
        "def test_detection_model(model, dataloader, class_names, th_score, th_iou, result_dir, SAVE_OPT, SAVE_FULL, VERBOSE, batch_size=1):\n",
        "    since = time.time()\n",
        "    # Standard values for anchors and image size\n",
        "    bins=[48,96,192,384]\n",
        "    size_anchors=[32,64,128,256,512]\n",
        "    min_size = 800\n",
        "    max_size = 1333\n",
        "    model.eval()   # Set model to evaluate mode\n",
        "\n",
        "    # Detection measurements\n",
        "    ret = 0\n",
        "    rel = np.zeros((len(class_names)-1,),dtype=int)\n",
        "    ret_rel = np.zeros((len(class_names)-1),dtype=int)\n",
        "\n",
        "    # Classification measurements\n",
        "    y_true=np.zeros((0,),dtype='int')\n",
        "    y_pred=np.zeros((0,),dtype='int')\n",
        "    y_score=np.zeros((0,),dtype='int')\n",
        "    batch_counter = 0\n",
        "    total_anchors=np.zeros((0,4),dtype='float32')\n",
        "    total_labels=np.zeros((0,),dtype=int)\n",
        "    print('Evaluating...')\n",
        "    with torch.no_grad():\n",
        "        # Iterate over data.\n",
        "        for inputs, targets, paths in dataloader:\n",
        "            batch_counter = batch_counter + 1\n",
        "            inputs = list(image.to(device) for image in inputs)\n",
        "            labels = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "            # Scale factor (useful for anchor extraction)\n",
        "            size_img=inputs[0].size()[1:]\n",
        "            scale_factor=min_size/min(size_img)\n",
        "            if max(size_img) * scale_factor > max_size:\n",
        "                scale_factor = max_size / max(size_img)\n",
        "            # Let's create the ground-truth boxes and labels\n",
        "            if (np.array(labels[0]['boxes'].cpu()).shape[0]==0):\n",
        "                gt_boxes=np.zeros((0,4),dtype='float32')\n",
        "            else:\n",
        "                gt_boxes=np.array(labels[0]['boxes'].detach().cpu())\n",
        "                gt_labels=np.array(labels[0]['labels'].detach().cpu())\n",
        "                # Relevant objects per class\n",
        "                for j in range(1,len(class_names)):\n",
        "                    rel[j-1]=rel[j-1]+np.sum(gt_labels==j)\n",
        "            # Prediction: returns boxes, scores (objectness), labels (class) and anchors\n",
        "            #print(device)\n",
        "            #print(next(model.parameters()).is_cuda)\n",
        "            pred = model(inputs)\n",
        "            pred[0]['scores']=pred[0]['scores'].detach().cpu()\n",
        "            pred[0]['boxes']=pred[0]['boxes'].detach().cpu()\n",
        "            pred[0]['labels']=pred[0]['labels'].detach().cpu()\n",
        "            pred[0]['anchors']=pred[0]['anchors'].detach().cpu()\n",
        "            # We use only those with scores>th_score and convert them to numpy arrays\n",
        "            if (len(pred[0]['scores'].numpy())==0):\n",
        "                pred_boxes=np.zeros((0,4),dtype='float32')\n",
        "                pred_labels=np.zeros((0,),dtype=int)\n",
        "            else:\n",
        "                pred_score = list(pred[0]['scores'].numpy())\n",
        "                if (pred_score[0]>th_score):\n",
        "                    pred_t = [pred_score.index(x) for x in pred_score if x>th_score][-1]\n",
        "                    pred_class = [class_names[i] for i in list(pred[0]['labels'].numpy())]\n",
        "                    pred_labels = pred[0]['labels'].numpy()\n",
        "                    pred_boxes = [[i[0], i[1], i[2], i[3]] for i in list(pred[0]['boxes'].numpy())]\n",
        "                    pred_boxes = np.array(pred_boxes[:pred_t+1])\n",
        "                    pred_anchors = [[i[0], i[1], i[2], i[3]] for i in list(pred[0]['anchors'].numpy())]\n",
        "                    pred_anchors = np.array(pred_anchors[:pred_t+1])\n",
        "                    total_anchors = np.concatenate((total_anchors,pred_anchors),axis=0)\n",
        "                    pred_class = pred_class[:pred_t+1]\n",
        "                    pred_labels = pred_labels[:pred_t+1]\n",
        "                    total_labels = np.concatenate((total_labels,pred_labels),axis=0)\n",
        "                else:\n",
        "                    pred_boxes=np.zeros((0,4),dtype='float32')\n",
        "                    pred_labels=np.zeros((0,),dtype=int)\n",
        "\n",
        "\n",
        "            # Retrieved objects\n",
        "            ret=ret+len(pred_labels)\n",
        "\n",
        "            # Detection statistics: we compute the intersection over union between the ground-truth objects\n",
        "            # and the retrieved ones, if it exceed th_iou, the detection is considered as a good one\n",
        "            for j in range(gt_boxes.shape[0]):\n",
        "                ious=np.zeros((pred_boxes.shape[0],),dtype='float')\n",
        "                for k in range(pred_boxes.shape[0]):\n",
        "                    ious[k]=bb_intersection_over_union(gt_boxes[j,:],pred_boxes[k,:])\n",
        "                if (len(ious)>0):\n",
        "                    iou_max=np.max(ious)\n",
        "                    pos=np.argmax(ious)\n",
        "                    if (iou_max>th_iou):\n",
        "                        ret_rel[gt_labels[j]-1]+=1\n",
        "                        y_true=np.concatenate((y_true,gt_labels[j][np.newaxis]),axis=0)\n",
        "                        y_pred=np.concatenate((y_pred,pred_labels[pos][np.newaxis]),axis=0)\n",
        "\n",
        "            # Save the images with detections and ground-truth objects in a new folder, to analyze the results\n",
        "            if SAVE_OPT and SAVE_FULL:\n",
        "                aux=paths[0].split('/')\n",
        "                folder_path=os.path.join(result_dir,'instances_pred_full')\n",
        "                if not os.path.exists(folder_path):\n",
        "                    os.makedirs(folder_path)\n",
        "                img=np.array(F.to_pil_image(inputs[0].cpu()))\n",
        "                for i in range(pred_boxes.shape[0]):\n",
        "                    cv2.rectangle(img, (int(pred_boxes[i][0]),int(pred_boxes[i][1])), (int(pred_boxes[i][2]),int(pred_boxes[i][3])),color=(255, 0, 0), thickness=1)\n",
        "                    cv2.putText(img,pred_class[i], (int(pred_boxes[i][0]),int(pred_boxes[i][1])), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255),thickness=1)\n",
        "                for i in range(gt_boxes.shape[0]):\n",
        "                    cv2.rectangle(img, (int(gt_boxes[i][0]),int(gt_boxes[i][1])), (int(gt_boxes[i][2]),int(gt_boxes[i][3])),color=(0, 255, 0), thickness=1)\n",
        "                    cv2.putText(img,class_names[gt_labels[i]], (int(gt_boxes[i][0]),int(gt_boxes[i][1])), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255),thickness=1)\n",
        "                pred_path=folder_path+'/'+aux[-1]\n",
        "                cv2.imwrite(pred_path,cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
        "            elif SAVE_OPT and not SAVE_FULL:\n",
        "                aux=paths[0].split('/')\n",
        "                folder_path=os.path.join(result_dir,'instances_pred')\n",
        "                if not os.path.exists(folder_path):\n",
        "                    os.makedirs(folder_path)\n",
        "                for i in range(pred_boxes.shape[0]):\n",
        "                    anchor_size=size_anchors[np.digitize(np.sqrt((pred_anchors[i,2]-pred_anchors[i,0])*(pred_anchors[i,3]-pred_anchors[i,1])),bins)]\n",
        "                    img=np.array(F.to_pil_image(inputs[0].cpu()))\n",
        "                    cv2.rectangle(img, (int(pred_boxes[i][0]),int(pred_boxes[i][1])), (int(pred_boxes[i][2]),int(pred_boxes[i][3])),color=(255, 0, 0), thickness=1)\n",
        "                    cv2.rectangle(img, (int(pred_anchors[i][0]/scale_factor),int(pred_anchors[i][1]/scale_factor)), (int(pred_anchors[i][2]/scale_factor),int(pred_anchors[i][3]/scale_factor)), color=(0, 0, 255), thickness=1)\n",
        "                    cv2.putText(img,pred_class[i], (int(pred_boxes[i][0]),int(pred_boxes[i][1])), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255),thickness=1)\n",
        "                    for j in range(gt_boxes.shape[0]):\n",
        "                        cv2.rectangle(img, (int(gt_boxes[j][0]),int(gt_boxes[j][1])), (int(gt_boxes[j][2]),int(gt_boxes[j][3])),color=(0, 255, 0), thickness=1)\n",
        "                        cv2.putText(img,class_names[gt_labels[j]], (int(gt_boxes[j][0]),int(gt_boxes[j][1])), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255),thickness=1)\n",
        "                    if (not os.path.exists(os.path.join(folder_path,str(anchor_size)))):\n",
        "                        os.makedirs(os.path.join(folder_path,str(anchor_size)))\n",
        "                    pred_path=folder_path+'/'+str(anchor_size)+'/'+aux[-1][:-4]+'_'+str(i)+'.png'\n",
        "                    cv2.imwrite(pred_path,cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    # Detection statistics: precision and recall per class\n",
        "    if (ret>0):\n",
        "        precision_RPN=np.sum(ret_rel)/ret\n",
        "    else:\n",
        "        precision_RPN=0\n",
        "    recall_RPN=np.zeros((rel.size,),dtype='float32')\n",
        "    for j in range(rel.size):\n",
        "        if (rel[j]>0):\n",
        "            recall_RPN[j]=ret_rel[j]/rel[j]\n",
        "        else:\n",
        "            recall_RPN[j]=0\n",
        "    # F1 score(weighted mean of precision and recall)\n",
        "    if (np.mean(recall_RPN)==0 or precision_RPN==0):\n",
        "        f1_score_RPN=0\n",
        "    else:\n",
        "        f1_score_RPN=2*np.mean(recall_RPN)*precision_RPN/(np.mean(recall_RPN)+precision_RPN)\n",
        "    # Classification statistics: precision and recall\n",
        "    prec_rec_marginal=precision_recall_fscore_support(y_true, y_pred, average=None,labels=[f for f in range(1,len(class_names))],zero_division=0)\n",
        "    prec_rec_global=precision_recall_fscore_support(y_true, y_pred, average='macro',labels=[f for f in range(1,len(class_names))],zero_division=0)\n",
        "    # Confusion matrix\n",
        "    cm_global=confusion_matrix(y_true, y_pred, labels=None, sample_weight=None, normalize=None)\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Evaluation complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Objectness-RPN. F1: {:4f}.     Precision: {:4f}. Recall: {:4f}'.format(f1_score_RPN,precision_RPN,np.mean(recall_RPN)))\n",
        "    if (VERBOSE):\n",
        "        for i in range(1,len(class_names)):\n",
        "            print('Class: ' + class_names[i]+'. Recall: {:1d}/{:1d}'.format(ret_rel[i-1],rel[i-1]))\n",
        "        print('')\n",
        "    if ((prec_rec_global[0]+prec_rec_global[1])>0):\n",
        "        f1_class=2*prec_rec_global[0]*prec_rec_global[1]/(prec_rec_global[0]+prec_rec_global[1])\n",
        "    else:\n",
        "        f1_class=0\n",
        "    print('Global classification: F1: {:4f}.     Precision: {:4f}. Recall: {:4f}'.format(f1_class,prec_rec_global[0],prec_rec_global[1]))\n",
        "    if (VERBOSE):\n",
        "        for i in range(1,len(class_names)):\n",
        "            if ((prec_rec_marginal[0][i-1]+prec_rec_marginal[1][i-1])>0):\n",
        "                f1_class=2*prec_rec_marginal[0][i-1]*prec_rec_marginal[1][i-1]/(prec_rec_marginal[0][i-1]+prec_rec_marginal[1][i-1])\n",
        "            else:\n",
        "                f1_class=0\n",
        "            print('Class: ' + class_names[i]+'. F1: {:4f}.     Precision: {:4f}. Recall: {:4f}'.format(f1_class,prec_rec_marginal[0][i-1],prec_rec_marginal[1][i-1]))\n",
        "    return (precision_RPN,recall_RPN,f1_score_RPN,cm_global,prec_rec_global,prec_rec_marginal,total_anchors,total_labels)"
      ],
      "metadata": {
        "id": "nSNNPs8qg7cT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento"
      ],
      "metadata": {
        "id": "g59uu860hXpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from engine import train_one_epoch, eval_one_epoch\n",
        "import time\n",
        "\n",
        "# We create the baseline folder\n",
        "result_dir='baseline_results'\n",
        "if not os.path.exists(result_dir):\n",
        "    os.makedirs(result_dir)\n",
        "\n",
        "# Weights for L_objectness, L_reg, L_cls y L_reg2\n",
        "weights=[1,1,1,1]\n",
        "\n",
        "# Training\n",
        "# construct an optimizer\n",
        "params = [p for p in model_ft.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=lr,\n",
        "                                momentum=0.9, weight_decay=0.0005)\n",
        "# and a learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                                step_size=step_size,\n",
        "                                                gamma=0.1)\n",
        "\n",
        "# CSV file for training results\n",
        "if os.path.exists(os.path.join(result_dir,'log.csv')):\n",
        "    os.remove(os.path.join(result_dir,'log.csv'))\n",
        "csv_file=open(os.path.join(os.path.join(result_dir,'log.csv')),'w')\n",
        "coord_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "coord_writer.writerow(['Train total','Train rpn_box_reg','Train objectness','Train box_reg','Train classifier',\n",
        "                        'Val total','Val rpn_box_reg','Val objectness','Val box_reg','Val classifier'])\n",
        "best_f1=0\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "        #     train for one epoch, printing every epoch\n",
        "        train_aux_losses=train_one_epoch(model_ft, optimizer, data_loader, device, weights, epoch, print_freq=250)\n",
        "        # Update the learning rate\n",
        "        lr_scheduler.step()\n",
        "        # Validation\n",
        "        val_aux_losses=eval_one_epoch(model_ft, data_loader_test, device, epoch, print_freq=250)\n",
        "        # Save the losses\n",
        "        coord_writer.writerow([str(train_aux_losses['total']),str(train_aux_losses['rpn_box_reg']),str(train_aux_losses['objectness']),\n",
        "                                str(train_aux_losses['box_reg']),str(train_aux_losses['classifier']),\n",
        "                                str(val_aux_losses['total']),str(val_aux_losses['rpn_box_reg']),str(val_aux_losses['objectness']),\n",
        "                                str(val_aux_losses['box_reg']),str(val_aux_losses['classifier'])])\n",
        "        # Evaluation\n",
        "        [precision,recall,f1_score,cm,total,partial,,]=test_detection_model(model_ft, data_loader_test, class_names, 0.5,0.5, result_dir, False, False, False)\n",
        "        # Save the state and the model with best AP-score for inference\n",
        "        state = {'epoch': epoch + 1, 'state_dict': model_ft.state_dict(),\n",
        "                  'optimizer': optimizer.state_dict(),\n",
        "                  'scheduler':lr_scheduler.state_dict(),}\n",
        "        torch.save(state, os.path.join(result_dir,'faster-rcnn-epoch{}.pth'.format(epoch)))\n",
        "        if (f1_score>best_f1):\n",
        "            torch.save(state, os.path.join(result_dir,'best_model.pth'.format(epoch)))\n",
        "            best_f1=f1_score\n",
        "        \"\"\"else:\n",
        "        # Load this epoch information to resume training\n",
        "        print(\"=> loading checkpoint '{}'\".format(epoch))\n",
        "        checkpoint = torch.load(os.path.join(result_dir,'faster-rcnn-epoch{}.pth'.format(epoch)))\n",
        "        lr_scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "        model_ft.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        print(\"=> loaded checkpoint '{}'\" .format(epoch))\"\"\"\n",
        "\n",
        "csv_file.close()"
      ],
      "metadata": {
        "id": "D3R6YZzVhTvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluación"
      ],
      "metadata": {
        "id": "ABDv19C8hlpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference\n",
        "model_weights = torch.load(os.path.join(result_dir,'best_model.pth'.format(num_epochs-1)))['state_dict']\n",
        "model_ft.load_state_dict(model_weights)\n",
        "_=test_detection_model(model_ft, data_loader_test, class_names, 0.5, 0.5, result_dir, True, False, True)"
      ],
      "metadata": {
        "id": "FDH2WNSLhk1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experimentación"
      ],
      "metadata": {
        "id": "I_voSW1ZKArh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Influencia de los umbrales en la inferencia."
      ],
      "metadata": {
        "id": "mMKP4ZVmLYad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Include your code here\n",
        "print('TH_SCORE=0.3')\n",
        "_=test_detection_model(model_ft, data_loader_test, class_names, 0.3, 0.5, result_dir, False, False, True)\n",
        "print('\\nTH_SCORE=0.5')\n",
        "_=test_detection_model(model_ft, data_loader_test, class_names, 0.5, 0.5, result_dir, False, False, True)\n",
        "print('\\nTH_SCORE=0.7')\n",
        "_=test_detection_model(model_ft, data_loader_test, class_names, 0.7, 0.5, result_dir, False, False, True)"
      ],
      "metadata": {
        "id": "Nq-a_9uvKAfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Conclusiones\n",
        "* A medida que aumentamos TH_SCORE, la precisión del RPN mejora, pero a costa de perder recall.\n",
        "\n",
        "* Sin embargo, el clasificador global mejora ligeramente en F1 y precisión con valores más altos (0.7), lo que indica que es capaz de compensar la menor cantidad de propuestas con una clasificación más precisa.\n",
        "\n",
        "* La clase Truck sigue siendo la más débil en todas las métricas, lo que sugiere revisar su número de muestras o calidad de anotaciones."
      ],
      "metadata": {
        "id": "ABQBaps5LKqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Análisis del tamaño de los anchors"
      ],
      "metadata": {
        "id": "tmWOTG0XLi12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Include your code here\n",
        "import matplotlib.pyplot as plt\n",
        "from external import get_gt_anchors\n",
        "# define training and test data loaders\n",
        "data_loader_0 = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=batch_size, shuffle=True, num_workers=0,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "data_loader_test_0 = torch.utils.data.DataLoader(\n",
        "    dataset_test, batch_size=batch_size, shuffle=False, num_workers=0,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "# TRAIN\n",
        "ar, labels, size_anchors=get_gt_anchors(data_loader_0)\n",
        "fig, axes = plt.subplots(nrows=2, ncols=2,figsize=(15,15))\n",
        "ax0, ax1, ax2, ax3 = axes.flatten()\n",
        "ax0.hist([ar[np.where(labels==1)[0]],ar[np.where(labels==2)[0]],ar[np.where(labels==3)[0]],ar[np.where(labels==4)[0]]],color=['blue','orange','green','purple'],label=class_names[1:])\n",
        "ax0.legend(prop={'size': 10})\n",
        "ax0.set_title('Aspect ratio, train')\n",
        "ax1.hist([size_anchors[np.where(labels==1)[0]],size_anchors[np.where(labels==2)[0]],size_anchors[np.where(labels==3)[0]],size_anchors[np.where(labels==4)[0]]],color=['blue','orange','green','purple'],label=class_names[1:])\n",
        "ax1.legend(prop={'size': 10})\n",
        "ax1.set_title('Box size, train')\n",
        "\n",
        "# TEST\n",
        "ar, labels, size_anchors=get_gt_anchors(data_loader_test_0)\n",
        "ax2.hist([ar[np.where(labels==1)[0]],ar[np.where(labels==2)[0]],ar[np.where(labels==3)[0]],ar[np.where(labels==4)[0]]],color=['blue','orange','green','purple'],label=class_names[1:])\n",
        "ax2.legend(prop={'size': 10})\n",
        "ax2.set_title('Aspect ratio, test')\n",
        "ax3.hist([size_anchors[np.where(labels==1)[0]],size_anchors[np.where(labels==2)[0]],size_anchors[np.where(labels==3)[0]],size_anchors[np.where(labels==4)[0]]],color=['blue','orange','green','purple'],label=class_names[1:])\n",
        "ax3.legend(prop={'size': 10})\n",
        "ax3.set_title('Box size, test')\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pwv3m1DfKXWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Conclusiones\n",
        "* Los histogramas muestran una gran concentración de objetos pequeños (especialmente coches y motos), lo que sugiere que los anchors pequeños están más representados.\n",
        "\n",
        "* El aspect ratio dominante está en torno a 1–2 para todas las clases, aunque los vehículos como autobuses o camiones tienden a ratios más alargados.\n"
      ],
      "metadata": {
        "id": "IsiZXuPXL8JO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. _RoI pooling_ y clasificación\n"
      ],
      "metadata": {
        "id": "VLtPWvFIMsLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from external import visualize_roipooling\n",
        "\n",
        "# For the roi-pooling experiment, analyze the top bounding-boxes (th_score>0.7) and only those with\n",
        "# th_iou>0.7 with the ground-truth\n",
        "visualize_roipooling(model_ft, data_loader_test, device, class_names, 0.7, 0.7, result_dir, True)\n",
        "# For the confusion matrix, experiment with the standard thresholds\n",
        "[precision,recall,ap_score,cm,_,_,_,_]=test_detection_model(model_ft, data_loader_test, class_names, 0.5, 0.5, result_dir, True, False, True)\n",
        "dconf=ConfusionMatrixDisplay(cm,display_labels=class_names[1:])\n",
        "dconf.plot()\n"
      ],
      "metadata": {
        "id": "8eXP1c7rKg2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Conclusiones\n",
        "\n",
        "* Aunque el F1 del RPN (~0.51) es razonable, hay margen de mejora, especialmente en recall para clases minoritarias como Truck (0.400) o Bus (0.61).\n",
        "\n",
        "* Esto indica que algunos objetos no están siendo bien propuestos por el RPN, posiblemente por un mismatch entre anchors predefinidos y tamaños reales.\n",
        "\n",
        "* A pesar de ese gap del RPN, el clasificador ROI corrige muchos errores, logrando F1 > 0.92 en clases como Bus y Motorcycle.\n",
        "\n",
        "* Sin embargo, Truck sigue siendo la clase más débil (F1 = 0.48), probablemente por menos datos o variedad."
      ],
      "metadata": {
        "id": "ZG9uo13DMJ54"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Funciones de pérdida"
      ],
      "metadata": {
        "id": "6vKYVakkM1Hy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def zero_features_hook(self, input, output):\n",
        "    output=torch.zeros_like(output)\n",
        "    return output\n",
        "\n",
        "def zero_features_tuple_hook(self, input, output):\n",
        "    output=(output[0],torch.zeros_like(output[1]))\n",
        "    return output\n",
        "\n",
        "def get_model_detection_no_bbox_regression(num_classes):\n",
        "    # load a Faster RCNN model pre-trained on COCO\n",
        "    model = torchvision_05.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "    # get number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    # Regression outputs to 0\n",
        "    model.rpn.head.bbox_pred.register_forward_hook(zero_features_hook)\n",
        "    model.roi_heads.box_predictor.register_forward_hook(zero_features_tuple_hook)\n",
        "\n",
        "    return model\n",
        "\n",
        "model_ft = get_model_detection_no_bbox_regression(num_classes)\n",
        "# Send the model to GPU\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# We create the no_regression folder\n",
        "result_dir='no_regression'\n",
        "if not os.path.exists(result_dir):\n",
        "    os.makedirs(result_dir)\n",
        "\n",
        "# Weights for L_objectness, L_reg, L_cls y L_reg2\n",
        "weights=[1,0,1,0]\n",
        "\n",
        "# Training\n",
        "# construct an optimizer\n",
        "params = [p for p in model_ft.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=lr,\n",
        "                                momentum=0.9, weight_decay=0.0005)\n",
        "# and a learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                                step_size=step_size,\n",
        "                                                gamma=0.1)\n",
        "\n",
        "# CSV file for training results\n",
        "if os.path.exists(os.path.join(result_dir,'log.csv')):\n",
        "    os.remove(os.path.join(result_dir,'log.csv'))\n",
        "csv_file=open(os.path.join(os.path.join(result_dir,'log.csv')),'w')\n",
        "coord_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "coord_writer.writerow(['Train total','Train rpn_box_reg','Train objectness','Train box_reg','Train classifier',\n",
        "                        'Val total','Val rpn_box_reg','Val objectness','Val box_reg','Val classifier'])\n",
        "best_f1=0\n",
        "for epoch in range(num_epochs):\n",
        "    if not os.path.exists(os.path.join(result_dir,'faster-rcnn-epoch{}.pth'.format(epoch))):\n",
        "        #     train for one epoch, printing every epoch\n",
        "        train_aux_losses=train_one_epoch(model_ft, optimizer, data_loader, device, weights, epoch, print_freq=250)\n",
        "        # Update the learning rate\n",
        "        lr_scheduler.step()\n",
        "        # Validation\n",
        "        val_aux_losses=eval_one_epoch(model_ft, data_loader_test, device, epoch, print_freq=250)\n",
        "        # Update measurement file\n",
        "        coord_writer.writerow([str(train_aux_losses['total']),str(train_aux_losses['rpn_box_reg']),str(train_aux_losses['objectness']),\n",
        "                                str(train_aux_losses['box_reg']),str(train_aux_losses['classifier']),\n",
        "                                str(val_aux_losses['total']),str(val_aux_losses['rpn_box_reg']),str(val_aux_losses['objectness']),\n",
        "                                str(val_aux_losses['box_reg']),str(val_aux_losses['classifier'])])\n",
        "        # Evaluation\n",
        "        [precision,recall,f1_score,cm,total,partial,_,_]=test_detection_model(model_ft, data_loader_test, class_names, 0.5, 0.5, result_dir, False, False, False)\n",
        "        # Save the state and the model with best AP-score for inference\n",
        "        state = {'epoch': epoch + 1, 'state_dict': model_ft.state_dict(),\n",
        "                  'optimizer': optimizer.state_dict(),\n",
        "                  'scheduler':lr_scheduler.state_dict(), }\n",
        "        torch.save(state, os.path.join(result_dir,'faster-rcnn-epoch{}.pth'.format(epoch)))\n",
        "        if (f1_score>best_f1):\n",
        "            torch.save(state, os.path.join(result_dir,'best_model.pth'.format(epoch)))\n",
        "            best_f1=f1_score\n",
        "    else:\n",
        "        # Load this epoch information to resume training\n",
        "        print(\"=> loading checkpoint '{}'\".format(epoch))\n",
        "        checkpoint = torch.load(os.path.join(result_dir,'faster-rcnn-epoch{}.pth'.format(epoch)))\n",
        "        lr_scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "        model_ft.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        print(\"=> loaded checkpoint '{}'\" .format(epoch))\n",
        "\n",
        "csv_file.close()"
      ],
      "metadata": {
        "id": "4FDLT9QGKo07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference\n",
        "model_weights = torch.load(os.path.join(result_dir,'best_model.pth'.format(num_epochs-1)))['state_dict']\n",
        "model_ft.load_state_dict(model_weights)\n",
        "_=test_detection_model(model_ft, data_loader_test, class_names, 0.5, 0.5, result_dir, True, False, True)"
      ],
      "metadata": {
        "id": "OFBFmSnQKsb5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}